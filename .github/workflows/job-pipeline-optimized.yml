name: Job Pipeline v3.0 - Optimized
# =====================================
# æ·±åº¦ä¼˜åŒ–ï¼šå¢é‡æ›´æ–° + å¿«é€Ÿå»é‡ + 3æ¬¡/å¤©è¿è¡Œ

on:
  schedule:
    # æ¯å¤©3æ¬¡ï¼šæ—©ä¸­æ™š (UTCæ—¶é—´)
    # è·å…°æ—¶é—´: 09:00 / 14:00 / 19:00 (CET) æˆ– 10:00 / 15:00 / 20:00 (CEST)
    - cron: '0 8,13,18 * * 1-5'
    # å‘¨æœ«ä¹Ÿè·‘ä¸€æ¬¡ï¼Œé˜²æ­¢é”™è¿‡å‘¨äº”æ™šä¸Šå‘å¸ƒçš„èŒä½
    - cron: '0 9 * * 0,6'

  workflow_dispatch:
    inputs:
      profile:
        description: 'Search profile (ml_data / backend_data / quant / all)'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - ml_data
          - backend_data
          - quant
      skip_scrape:
        description: 'Skip scraping (use existing data)'
        required: false
        default: false
        type: boolean
      force_refresh:
        description: 'Force refresh (ignore cache, re-scrape all)'
        required: false
        default: false
        type: boolean
      ai_limit:
        description: 'Max jobs for AI analysis (empty = auto)'
        required: false
        default: ''
        type: string

env:
  PYTHONUNBUFFERED: '1'
  TURSO_DATABASE_URL: ${{ secrets.TURSO_DATABASE_URL }}
  TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
  ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
  ANTHROPIC_BASE_URL: ${{ secrets.ANTHROPIC_BASE_URL }}
  # æ€§èƒ½ä¼˜åŒ–ï¼šå‡å°‘åŒæ­¥é¢‘ç‡
  TURSO_SYNC_INTERVAL: '300'  # 5åˆ†é’ŸåŒæ­¥ä¸€æ¬¡

jobs:
  # ==========================================
  # Job 1: å¿«é€Ÿçˆ¬å– (å¢é‡æ›´æ–°)
  # ==========================================
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30  # çˆ¬å–æ—¶é—´é™åˆ¶ï¼š30åˆ†é’Ÿï¼ˆLinkedInå¯èƒ½è¾ƒæ…¢ï¼‰
    outputs:
      new_jobs_count: ${{ steps.scrape.outputs.new_jobs_count }}
      total_jobs_count: ${{ steps.scrape.outputs.total_jobs_count }}
      duration_seconds: ${{ steps.scrape.outputs.duration_seconds }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          playwright install chromium

      - name: Write LinkedIn cookies
        env:
          COOKIES_JSON: ${{ secrets.LINKEDIN_COOKIES }}
        run: |
          mkdir -p config
          printf '%s' "$COOKIES_JSON" > config/linkedin_cookies.json

      - name: Scrape LinkedIn (Incremental)
        id: scrape
        run: |
          PROFILE="${{ github.event.inputs.profile || 'all' }}"
          FORCE_FLAG=""
          if [ "${{ github.event.inputs.force_refresh }}" == "true" ]; then
            FORCE_FLAG="--force-refresh"
            echo "ğŸ”„ Force refresh mode enabled"
          fi

          echo "=== Starting incremental scrape: $PROFILE ==="
          echo "Using linkedin_scraper_v6.py with database deduplication"
          START_TIME=$(date +%s)

          # ä½¿ç”¨ v6 çˆ¬å–ï¼ˆå·²æœ‰æ•°æ®åº“å»é‡ï¼‰
          # "all" ä¸æ˜¯çœŸå® profile åï¼Œä¸ä¼  --profile è®©è„šæœ¬è‡ªåŠ¨è¿è¡Œæ‰€æœ‰ enabled profiles
          if [ "$PROFILE" = "all" ]; then
            python scripts/linkedin_scraper_v6.py \
              --headless \
              --save-to-db \
              --no-json \
              $FORCE_FLAG
          else
            python scripts/linkedin_scraper_v6.py \
              --profile "$PROFILE" \
              --headless \
              --save-to-db \
              --no-json \
              $FORCE_FLAG
          fi
          
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          
          # ä» scraper è¾“å‡ºçš„ metrics æ–‡ä»¶è¯»å–çœŸå®æ–°å¢æ•°
          NEW_JOBS=$(python -c "import json; m=json.load(open('data/scrape_metrics.json')); print(m.get('new_jobs', 0))")
          echo "new_jobs_count=$NEW_JOBS" >> $GITHUB_OUTPUT
          echo "duration_seconds=$DURATION" >> $GITHUB_OUTPUT
          echo "âœ… Scraped in ${DURATION}s, new jobs: $NEW_JOBS"

      - name: Upload scrape stats
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-stats-${{ github.run_id }}
          path: data/scrape_metrics.json
          retention-days: 7

  # ==========================================
  # Job 2: è§„åˆ™è¯„åˆ† (å¿«é€Ÿæœ¬åœ°å¤„ç†)
  # ==========================================
  score:
    runs-on: ubuntu-latest
    needs: scrape
    # å³ä½¿æ²¡æœ‰æ–°èŒä½ä¹Ÿè¿è¡Œï¼ˆå¯èƒ½æœ‰ä¹‹å‰æœªå¤„ç†çš„ï¼‰
    timeout-minutes: 10

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Rule-based scoring
        run: |
          echo "=== Running rule-based scoring ==="
          python scripts/job_pipeline.py --process

      - name: Scoring stats
        run: python scripts/job_pipeline.py --stats

  # ==========================================
  # Job 3: AIåˆ†æ (åªå¤„ç†é«˜åˆ†æ–°èŒä½)
  # ==========================================
  ai-analyze:
    runs-on: ubuntu-latest
    needs: [scrape, score]
    # Always run if scrape succeeded â€” handles both new jobs and backlog
    if: always() && needs.scrape.result == 'success'
    timeout-minutes: 30  # AIåˆ†æé™åˆ¶30åˆ†é’Ÿ

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: AI Analysis (Prioritized)
        run: |
          LIMIT="${{ github.event.inputs.ai_limit }}"
          
          # å¦‚æœæ²¡æœ‰æŒ‡å®šé™åˆ¶ï¼Œç»™è¶³å¤Ÿçš„åé¢å¤„ç†æ–°èŒä½ + ç§¯å‹
          if [ -z "$LIMIT" ]; then
            NEW_JOBS="${{ needs.scrape.outputs.new_jobs_count }}"
            # æ–°èŒä½ + æœ€å¤š20ä¸ªç§¯å‹èŒä½
            LIMIT=$((NEW_JOBS + 20))
            echo "Auto-limit: $LIMIT (new: $NEW_JOBS + backlog: 20)"
          fi
          
          echo "=== Running AI analysis (limit: $LIMIT) ==="
          python scripts/job_pipeline.py --ai-analyze --limit "$LIMIT"

      - name: AI analysis stats
        run: python scripts/job_pipeline.py --stats

  # ==========================================
  # Job 4: é€šçŸ¥æ±‡æ€»
  # ==========================================
  notify:
    runs-on: ubuntu-latest
    needs: [scrape, score, ai-analyze]
    if: always() && needs.scrape.result == 'success'
    timeout-minutes: 2

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Send Telegram notification
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
        run: python scripts/notify.py --status success

  # ==========================================
  # Job 5: å¥åº·æ£€æŸ¥ (æ€»æ˜¯è¿è¡Œ)
  # ==========================================
  health-check:
    runs-on: ubuntu-latest
    needs: [scrape, score, ai-analyze]
    if: always()
    timeout-minutes: 2

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Pipeline health check
        run: |
          echo "=== Pipeline Health Check ==="
          python scripts/job_pipeline.py --stats

      - name: Final stats
        run: python scripts/job_pipeline.py --stats
