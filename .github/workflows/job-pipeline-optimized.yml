name: Job Pipeline v3.0 - Optimized
# =====================================
# æ·±åº¦ä¼˜åŒ–ï¼šå¢é‡æ›´æ–° + å¿«é€Ÿå»é‡ + 3æ¬¡/å¤©è¿è¡Œ

on:
  schedule:
    # æ¯å¤©3æ¬¡ï¼šæ—©ä¸­æ™š (UTCæ—¶é—´)
    # è·å…°æ—¶é—´: 09:00 / 14:00 / 19:00 (CET) æˆ– 10:00 / 15:00 / 20:00 (CEST)
    - cron: '0 8,13,18 * * 1-5'
    # å‘¨æœ«ä¹Ÿè·‘ä¸€æ¬¡ï¼Œé˜²æ­¢é”™è¿‡å‘¨äº”æ™šä¸Šå‘å¸ƒçš„èŒä½
    - cron: '0 9 * * 0,6'

  workflow_dispatch:
    inputs:
      profile:
        description: 'Search profile (ml_data / backend_data / quant / all)'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - ml_data
          - backend_data
          - quant
      skip_scrape:
        description: 'Skip scraping (use existing data)'
        required: false
        default: false
        type: boolean
      force_refresh:
        description: 'Force refresh (ignore cache, re-scrape all)'
        required: false
        default: false
        type: boolean
      ai_limit:
        description: 'Max jobs for AI analysis (empty = auto)'
        required: false
        default: ''
        type: string

env:
  PYTHONUNBUFFERED: '1'
  TURSO_DATABASE_URL: ${{ secrets.TURSO_DATABASE_URL }}
  TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
  ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
  ANTHROPIC_BASE_URL: ${{ secrets.ANTHROPIC_BASE_URL }}
  # æ€§èƒ½ä¼˜åŒ–ï¼šå‡å°‘åŒæ­¥é¢‘ç‡
  TURSO_SYNC_INTERVAL: '300'  # 5åˆ†é’ŸåŒæ­¥ä¸€æ¬¡

jobs:
  # ==========================================
  # Job 1: å¿«é€Ÿçˆ¬å– (å¢é‡æ›´æ–°)
  # ==========================================
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30  # çˆ¬å–æ—¶é—´é™åˆ¶ï¼š30åˆ†é’Ÿï¼ˆLinkedInå¯èƒ½è¾ƒæ…¢ï¼‰
    outputs:
      new_jobs_count: ${{ steps.scrape.outputs.new_jobs_count }}
      total_jobs_count: ${{ steps.scrape.outputs.total_jobs_count }}
      duration_seconds: ${{ steps.scrape.outputs.duration_seconds }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          playwright install chromium

      - name: Write LinkedIn cookies
        env:
          COOKIES_JSON: ${{ secrets.LINKEDIN_COOKIES }}
        run: |
          mkdir -p config
          printf '%s' "$COOKIES_JSON" > config/linkedin_cookies.json

      - name: Scrape LinkedIn (Incremental)
        id: scrape
        run: |
          PROFILE="${{ github.event.inputs.profile || 'all' }}"
          FORCE_FLAG=""
          if [ "${{ github.event.inputs.force_refresh }}" == "true" ]; then
            FORCE_FLAG="--force-refresh"
            echo "ğŸ”„ Force refresh mode enabled"
          fi

          echo "=== Starting incremental scrape: $PROFILE ==="
          echo "Note: Using 24h time filter (LinkedIn limitation), limiting to 4 pages per profile"
          START_TIME=$(date +%s)
          
          # ä½¿ç”¨ä¼˜åŒ–åçš„å¢é‡çˆ¬å–è„šæœ¬
          # --max-pages 4 = æ¯profileæœ€å¤š4é¡µ (~100èŒä½)
          python scripts/scraper_incremental.py \
            --profile "$PROFILE" \
            --headless \
            --max-pages 4 \
            $FORCE_FLAG \
            --output stats.json
          
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          
          # è¾“å‡ºç»Ÿè®¡
          if [ -f stats.json ]; then
            NEW_JOBS=$(python -c "import json; print(json.load(open('stats.json')).get('new_jobs', 0))")
            TOTAL_JOBS=$(python -c "import json; print(json.load(open('stats.json')).get('total_scraped', 0))")
            echo "new_jobs_count=$NEW_JOBS" >> $GITHUB_OUTPUT
            echo "total_jobs_count=$TOTAL_JOBS" >> $GITHUB_OUTPUT
            echo "duration_seconds=$DURATION" >> $GITHUB_OUTPUT
            echo "âœ… Scraped $NEW_JOBS new jobs (total: $TOTAL_JOBS) in ${DURATION}s"
          else
            echo "new_jobs_count=0" >> $GITHUB_OUTPUT
            echo "total_jobs_count=0" >> $GITHUB_OUTPUT
            echo "duration_seconds=$DURATION" >> $GITHUB_OUTPUT
          fi

      - name: Upload scrape stats
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-stats-${{ github.run_id }}
          path: stats.json
          retention-days: 7

  # ==========================================
  # Job 2: è§„åˆ™è¯„åˆ† (å¿«é€Ÿæœ¬åœ°å¤„ç†)
  # ==========================================
  score:
    runs-on: ubuntu-latest
    needs: scrape
    # å³ä½¿æ²¡æœ‰æ–°èŒä½ä¹Ÿè¿è¡Œï¼ˆå¯èƒ½æœ‰ä¹‹å‰æœªå¤„ç†çš„ï¼‰
    timeout-minutes: 5

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Rule-based scoring
        run: |
          echo "=== Running rule-based scoring ==="
          python scripts/job_pipeline.py --process

      - name: Scoring stats
        run: python scripts/job_pipeline.py --stats --brief

  # ==========================================
  # Job 3: AIåˆ†æ (åªå¤„ç†é«˜åˆ†æ–°èŒä½)
  # ==========================================
  ai-analyze:
    runs-on: ubuntu-latest
    needs: [scrape, score]
    # åªæœ‰æœ‰æ–°èŒä½æ—¶æ‰è¿è¡ŒAIåˆ†æ
    if: needs.scrape.outputs.new_jobs_count != '0' || github.event.inputs.ai_limit != ''
    timeout-minutes: 30  # AIåˆ†æé™åˆ¶30åˆ†é’Ÿ

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: AI Analysis (Prioritized)
        run: |
          LIMIT="${{ github.event.inputs.ai_limit }}"
          
          # å¦‚æœæ²¡æœ‰æŒ‡å®šé™åˆ¶ï¼Œè‡ªåŠ¨è®¡ç®—ï¼šæ–°èŒä½ + ä¸€äº›ç§¯å‹çš„é«˜åˆ†èŒä½
          if [ -z "$LIMIT" ]; then
            NEW_JOBS="${{ needs.scrape.outputs.new_jobs_count }}"
            # æœ€å¤šå¤„ç†ï¼šæ–°èŒä½ + æœ€å¤š10ä¸ªç§¯å‹èŒä½
            LIMIT=$((NEW_JOBS + 10))
            echo "ğŸ¤– Auto-limit: $LIMIT (new: $NEW_JOBS + backlog: 10)"
          fi
          
          echo "=== Running AI analysis (limit: $LIMIT) ==="
          python scripts/job_pipeline.py --ai-analyze --limit "$LIMIT"

      - name: Generate resumes for high-scoring jobs
        run: |
          echo "=== Generating tailored resumes ==="
          python scripts/job_pipeline.py --generate --limit 20

      - name: AI analysis stats
        run: python scripts/job_pipeline.py --stats --brief

  # ==========================================
  # Job 4: é€šçŸ¥æ±‡æ€»
  # ==========================================
  notify:
    runs-on: ubuntu-latest
    needs: [scrape, score, ai-analyze]
    if: always() && (needs.scrape.outputs.new_jobs_count != '0' || needs.ai-analyze.result == 'success')
    timeout-minutes: 2

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Send Discord notification
        env:
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
        run: |
          echo "=== Sending notification ==="
          python scripts/notify_discord.py \
            --new-jobs "${{ needs.scrape.outputs.new_jobs_count }}" \
            --duration "${{ needs.scrape.outputs.duration_seconds }}" \
            --run-url "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

  # ==========================================
  # Job 5: å¥åº·æ£€æŸ¥ (æ€»æ˜¯è¿è¡Œ)
  # ==========================================
  health-check:
    runs-on: ubuntu-latest
    needs: [scrape, score, ai-analyze]
    if: always()
    timeout-minutes: 2

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Pipeline health check
        run: |
          echo "=== Pipeline Health Check ==="
          python scripts/job_pipeline.py --stats

      - name: Final stats
        run: python scripts/job_pipeline.py --stats
