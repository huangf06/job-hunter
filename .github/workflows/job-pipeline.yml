name: Job Pipeline

on:
  schedule:
    # Weekdays 3x daily (Netherlands time)
    - cron: '0 8 * * 1-5'   # 09:00 CET / 10:00 CEST
    - cron: '0 12 * * 1-5'  # 13:00 CET / 14:00 CEST
    - cron: '0 16 * * 1-5'  # 17:00 CET / 18:00 CEST

  workflow_dispatch:
    inputs:
      profile:
        description: 'Search profile (ml_data / backend_data / quant / all)'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - ml_data
          - backend_data
          - quant
      run_ai_analyze:
        description: 'Run AI analysis after scoring'
        required: false
        default: true
        type: boolean
      ai_limit:
        description: 'Max jobs to AI-analyze (empty = no limit)'
        required: false
        default: ''
        type: string

concurrency:
  group: job-pipeline
  cancel-in-progress: true

jobs:
  scrape-and-process:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    env:
      PYTHONUNBUFFERED: '1'
      TURSO_DATABASE_URL: ${{ secrets.TURSO_DATABASE_URL }}
      TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      ANTHROPIC_BASE_URL: ${{ secrets.ANTHROPIC_BASE_URL }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          playwright install --with-deps chromium

      - name: Write LinkedIn cookies
        env:
          COOKIES_JSON: ${{ secrets.LINKEDIN_COOKIES }}
        run: |
          mkdir -p config
          printf '%s' "$COOKIES_JSON" > config/linkedin_cookies.json

      # Scrape — no continue-on-error, failures are real failures
      - name: Scrape jobs
        id: scrape
        run: |
          PROFILES="${{ github.event.inputs.profile || 'all' }}"
          if [ "$PROFILES" = "all" ]; then
            python scripts/linkedin_scraper_v6.py --headless --save-to-db --no-json
          else
            python scripts/linkedin_scraper_v6.py --profile "$PROFILES" --headless --save-to-db --no-json
          fi

      # Process pipeline (import → filter → rule score)
      - name: Process pipeline
        timeout-minutes: 15
        run: python scripts/job_pipeline.py --process

      # AI analysis
      - name: AI analyze
        if: github.event.inputs.run_ai_analyze != 'false'
        timeout-minutes: 20
        run: |
          LIMIT="${{ github.event.inputs.ai_limit }}"
          if [ -n "$LIMIT" ]; then
            python scripts/job_pipeline.py --ai-analyze --limit "$LIMIT"
          else
            python scripts/job_pipeline.py --ai-analyze
          fi

      # Generate resumes for high-scoring jobs
      - name: Generate resumes
        if: github.event.inputs.run_ai_analyze != 'false'
        timeout-minutes: 10
        run: python scripts/job_pipeline.py --generate

      # Stats
      - name: Pipeline stats
        if: always()
        run: python scripts/job_pipeline.py --stats

      # Telegram notification — always runs
      - name: Notify (success)
        if: success()
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
        run: python scripts/notify.py --status success

      - name: Notify (failure)
        if: failure()
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
        run: |
          # Find which step failed
          FAILED_STEP=""
          if [ "${{ steps.scrape.outcome }}" = "failure" ]; then FAILED_STEP="Scrape"; fi
          python scripts/notify.py --status failure --failed-step "$FAILED_STEP"
