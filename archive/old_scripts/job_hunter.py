"""
Job Hunter - å®Œæ•´æ±‚èŒæµç¨‹
==========================

æµç¨‹: çˆ¬è™« â†’ ç¡¬æ€§ç­›é€‰ â†’ AIè¯„åˆ†(â‰¥6åˆ†) â†’ ç”Ÿæˆç®€å† â†’ è®°å½•

Usage:
    python job_hunter.py --scrape "data scientist" --max 10    # çˆ¬å–+å¤„ç†
    python job_hunter.py --process <file.json>                  # å¤„ç†å·²æœ‰æ•°æ®
    python job_hunter.py --daily                                 # æ¯æ—¥è‡ªåŠ¨è¿è¡Œ
"""

import json
import re
import asyncio
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict

# é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
LEADS_DIR = DATA_DIR / "leads"
OUTPUT_DIR = PROJECT_ROOT / "output"
TEMPLATES_DIR = PROJECT_ROOT / "templates"
ASSETS_DIR = PROJECT_ROOT / "assets"

# ç¡®ä¿ç›®å½•å­˜åœ¨
for d in [DATA_DIR, LEADS_DIR, OUTPUT_DIR]:
    d.mkdir(parents=True, exist_ok=True)


@dataclass
class JobResult:
    """èŒä½å¤„ç†ç»“æœ"""
    job_id: str
    title: str
    company: str
    url: str
    status: str  # rejected | scored | generated | error
    score: Optional[float] = None
    reject_reason: Optional[str] = None
    resume_path: Optional[str] = None
    ai_analysis: Optional[str] = None
    error_msg: Optional[str] = None
    processed_at: str = None
    
    def __post_init__(self):
        if self.processed_at is None:
            self.processed_at = datetime.now().isoformat()


class HardFilter:
    """ç¡¬æ€§ç­›é€‰ - ä¸€ç¥¨å¦å†³ï¼ˆé›¶tokenï¼‰"""
    
    REJECT_PATTERNS = {
        "dutch_required": [
            r"dutch\s*(required|mandatory|essential|native|fluent)",
            r"nederlands\s*(verplicht|vereist|moeder.*taal)",
            r"fluency\s+in\s+dutch",
        ],
        "experience_too_high": [
            r"(10|15|20)\+?\s*years?\s*(of\s*)?experience",
            r"minimum\s*(8|9)\+?\s*years",
            r"senior.*with\s*\d{2}\+?\s*years",
        ],
        "senior_level": [
            r"\b(principal|staff|director|vp|vice president|head of)\b",
            r"\bsenior\b.*\b(manager|director|lead|architect)\b",
        ],
        "wrong_role": [
            r"\b(frontend|backend|fullstack|devops|sre|security)\b",
            r"\b(android|ios|mobile)\s*(developer|engineer)",
            r"\b(frontend|ui|ux)\s*(developer|designer)",
        ],
    }
    
    @classmethod
    def check(cls, job: Dict) -> Tuple[bool, Optional[str]]:
        """ç¡¬æ€§ç­›é€‰æ£€æŸ¥"""
        text = f"{job.get('title', '')} {job.get('description', '')}".lower()
        
        for rule_name, patterns in cls.REJECT_PATTERNS.items():
            for pattern in patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    return False, rule_name
        
        return True, None


class ResumeGenerator:
    """ç®€å†ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.template = self._load_template()
    
    def _load_template(self) -> str:
        """åŠ è½½HTMLæ¨¡æ¿"""
        template_path = TEMPLATES_DIR / "resume.html"
        if not template_path.exists():
            raise FileNotFoundError(f"Template not found: {template_path}")
        with open(template_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def generate(self, job: Dict, ai_analysis: str) -> str:
        """ç”Ÿæˆå®šåˆ¶ç®€å†HTML"""
        html = self.template
        company = job.get('company', 'the company')
        title = job.get('title', '').lower()
        
        # æ ¹æ®AIåˆ†æç¡®å®šè§’è‰²ç±»å‹
        role_type = "data_scientist"
        if 'machine learning' in title or 'ml' in title:
            role_type = "ml_engineer"
        elif 'data engineer' in title:
            role_type = "data_engineer"
        elif 'quant' in title:
            role_type = "quantitative"
        elif 'analyst' in title:
            role_type = "data_analyst"
        
        # ç”ŸæˆSummary
        summaries = {
            "ml_engineer": f"Machine Learning Engineer with expertise in developing and deploying ML models at scale. M.Sc. in AI from VU Amsterdam with thesis on Uncertainty Quantification in Deep RL. Experienced in PyTorch, TensorFlow, and production ML systems. Eager to contribute to {company}'s ML initiatives.",
            
            "data_engineer": f"Data Engineer with strong background in building scalable data pipelines, ETL processes, and ML infrastructure. M.Sc. in AI from VU Amsterdam. Experienced in Python, SQL, PySpark, and cloud platforms. Seeking to leverage data engineering skills at {company}.",
            
            "quantitative": f"Quantitative Researcher with hands-on experience in factor research, backtesting, and live trading systems. Background in multi-factor alpha models and futures strategies with proven track record. M.Sc. in AI from VU Amsterdam. Seeking quantitative role at {company}.",
            
            "data_analyst": f"Data Analyst with strong foundation in statistical analysis, data visualization, and business intelligence. M.Sc. in AI from VU Amsterdam. Experienced in SQL, Python, and data-driven decision making. Excited to bring analytical skills to {company}.",
            
            "data_scientist": f"Data Scientist with expertise in machine learning, statistical modeling, and data-driven decision making. M.Sc. in AI from VU Amsterdam. Experienced in building end-to-end ML pipelines, credit risk modeling, and quantitative analysis. Excited to bring analytical skills to {company}."
        }
        
        summary = summaries.get(role_type, summaries["data_scientist"])
        
        # æ›¿æ¢summary
        html = re.sub(
            r'(<div class="tailored-summary"[^\u003e]*\u003e)(.*?)(\u003c/div\u003e)',
            f'\\1{summary}\\3',
            html,
            flags=re.DOTALL
        )
        
        return html
    
    def save_html(self, html: str, job: Dict) -> Path:
        """ä¿å­˜HTMLæ–‡ä»¶"""
        safe_company = re.sub(r'[^\w\-]', '_', job.get('company', 'unknown').lower())[:20]
        safe_title = re.sub(r'[^\w\-]', '_', job.get('title', 'job').lower())[:20]
        filename = f"Fei_Huang_{safe_company}_{safe_title}.html"
        filepath = OUTPUT_DIR / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(html)
        
        return filepath
    
    async def generate_pdf(self, html_path: Path) -> Path:
        """HTMLè½¬PDF"""
        from playwright.async_api import async_playwright
        
        pdf_path = html_path.with_suffix('.pdf')
        
        async with async_playwright() as p:
            browser = await p.chromium.launch()
            page = await browser.new_page()
            await page.goto(f"file:///{html_path.resolve()}")
            await page.wait_for_load_state("networkidle")
            await page.pdf(
                path=str(pdf_path),
                format="A4",
                margin={"top": "0.4in", "right": "0.4in", "bottom": "0.4in", "left": "0.4in"},
                print_background=True
            )
            await browser.close()
        
        return pdf_path


class JobTracker:
    """èŒä½è¿½è¸ªå™¨"""
    
    TRACKER_FILE = DATA_DIR / "applications.json"
    
    def __init__(self):
        self.data = self._load()
    
    def _load(self) -> Dict:
        """åŠ è½½è¿½è¸ªæ•°æ®"""
        default_data = {"applications": [], "stats": {"total": 0, "generated": 0, "rejected": 0}}
        if self.TRACKER_FILE.exists():
            try:
                with open(self.TRACKER_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if "stats" not in data:
                        data["stats"] = default_data["stats"]
                    return data
            except:
                return default_data
        return default_data
    
    def _save(self):
        """ä¿å­˜è¿½è¸ªæ•°æ®"""
        with open(self.TRACKER_FILE, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, indent=2, ensure_ascii=False)
    
    def add(self, result: JobResult):
        """æ·»åŠ è®°å½•"""
        self.data["applications"].append(asdict(result))
        self.data["stats"]["total"] += 1
        if result.status == "generated":
            self.data["stats"]["generated"] += 1
        elif result.status == "rejected":
            self.data["stats"]["rejected"] += 1
        self._save()
    
    def exists(self, job_id: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡"""
        return any(a["job_id"] == job_id for a in self.data["applications"])
    
    def get_pending_review(self) -> List[Dict]:
        """è·å–å¾…å®¡æ ¸çš„ç®€å†ï¼ˆå·²ç”ŸæˆPDFä½†æœªæŠ•é€’ï¼‰"""
        return [
            a for a in self.data["applications"]
            if a["status"] == "generated" and a.get("resume_path")
        ]


class JobHunter:
    """æ±‚èŒä¸»æ§"""
    
    def __init__(self):
        self.tracker = JobTracker()
        self.resume_gen = ResumeGenerator()
    
    def _generate_job_id(self, job: Dict) -> str:
        """ç”ŸæˆèŒä½å”¯ä¸€ID"""
        key = f"{job.get('title', '')}-{job.get('company', '')}-{job.get('url', '')}"
        return hashlib.md5(key.encode()).hexdigest()[:12]
    
    async def process_job(self, job: Dict, ai_scorer=None) -> JobResult:
        """
        å¤„ç†å•ä¸ªèŒä½
        
        Args:
            job: èŒä½ä¿¡æ¯
            ai_scorer: AIè¯„åˆ†å‡½æ•°ï¼ˆå¤–éƒ¨ä¼ å…¥ï¼‰
        """
        job_id = self._generate_job_id(job)
        
        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
        if self.tracker.exists(job_id):
            return JobResult(
                job_id=job_id,
                title=job.get('title', ''),
                company=job.get('company', ''),
                url=job.get('url', ''),
                status="skipped",
                reject_reason="already_processed"
            )
        
        result = JobResult(
            job_id=job_id,
            title=job.get('title', ''),
            company=job.get('company', ''),
            url=job.get('url', ''),
            status="pending"
        )
        
        try:
            # ========== ç¬¬ä¸€å±‚ï¼šç¡¬æ€§ç­›é€‰ ==========
            passed, reject_reason = HardFilter.check(job)
            if not passed:
                result.status = "rejected"
                result.reject_reason = reject_reason
                self.tracker.add(result)
                print(f"  âŒ REJECTED - {reject_reason}")
                return result
            
            # ========== ç¬¬äºŒå±‚ï¼šAIè¯„åˆ† ==========
            if ai_scorer:
                score, analysis = await ai_scorer(job)
                result.score = score
                result.ai_analysis = analysis
                
                if score < 6.0:
                    result.status = "scored"
                    result.reject_reason = f"score_too_low({score:.1f})"
                    self.tracker.add(result)
                    print(f"  âš ï¸  LOW SCORE - {score:.1f}")
                    return result
            
            # ========== ç¬¬ä¸‰å±‚ï¼šç”Ÿæˆç®€å† ==========
            print(f"  ğŸ“ GENERATING...")
            
            html = self.resume_gen.generate(job, result.ai_analysis or "")
            html_path = self.resume_gen.save_html(html, job)
            pdf_path = await self.resume_gen.generate_pdf(html_path)
            result.resume_path = str(pdf_path)
            result.status = "generated"
            
            self.tracker.add(result)
            print(f"  âœ… DONE - {pdf_path.name}")
            
        except Exception as e:
            result.status = "error"
            result.error_msg = str(e)
            self.tracker.add(result)
            print(f"  ğŸ’¥ ERROR - {e}")
        
        return result
    
    async def process_jobs(self, jobs: List[Dict], ai_scorer=None) -> List[JobResult]:
        """æ‰¹é‡å¤„ç†èŒä½"""
        results = []
        print(f"\n{'='*70}")
        print(f"Processing {len(jobs)} jobs...")
        print(f"{'='*70}\n")
        
        for i, job in enumerate(jobs, 1):
            company = job.get('company', 'Unknown')
            title = job.get('title', 'Unknown')[:40]
            print(f"[{i}/{len(jobs)}] {company} - {title}...")
            result = await self.process_job(job, ai_scorer)
            results.append(result)
        
        self._print_stats(results)
        return results
    
    def _print_stats(self, results: List[JobResult]):
        """æ‰“å°ç»Ÿè®¡"""
        total = len(results)
        rejected = sum(1 for r in results if r.status == "rejected")
        low_score = sum(1 for r in results if r.status == "scored")
        generated = sum(1 for r in results if r.status == "generated")
        errors = sum(1 for r in results if r.status == "error")
        skipped = sum(1 for r in results if r.status == "skipped")
        
        print(f"\n{'='*70}")
        print("PROCESSING COMPLETE")
        print(f"{'='*70}")
        print(f"Total:      {total}")
        print(f"Generated:  {generated} âœ…")
        print(f"Rejected:   {rejected} âŒ")
        print(f"Low score:  {low_score} âš ï¸")
        print(f"Skipped:    {skipped} â­ï¸")
        print(f"Errors:     {errors} ğŸ’¥")
        print(f"{'='*70}\n")
        
        if generated > 0:
            print(f"ğŸ“„ Generated resumes are in: {OUTPUT_DIR}")
            print(f"ğŸ“Š Tracker saved to: {self.tracker.TRACKER_FILE}")
            print(f"\nğŸ‘‰ Next step: Review generated PDFs and apply manually\n")


# ============== CLI ==============

def load_jobs_from_file(filepath: str) -> List[Dict]:
    """ä»æ–‡ä»¶åŠ è½½èŒä½"""
    path = Path(filepath)
    if not path.exists():
        path = LEADS_DIR / filepath
    
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
        return data.get('jobs', [])


async def scrape_linkedin(search_term: str, max_jobs: int = 10) -> List[Dict]:
    """çˆ¬å–LinkedInèŒä½"""
    # å¯¼å…¥çˆ¬è™«
    import sys
    sys.path.insert(0, str(PROJECT_ROOT / "scripts"))
    from linkedin_scraper import LinkedInJobScraper
    
    scraper = LinkedInJobScraper(headless=True)
    jobs = await scraper.scrape_search_results(
        search_term=search_term,
        location="Netherlands",
        max_jobs=max_jobs
    )
    
    # ä¿å­˜åŸå§‹æ•°æ®
    if jobs:
        scraper.save_jobs(jobs)
    
    return jobs


async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Job Hunter')
    parser.add_argument('--scrape', metavar='TERM', help='Scrape LinkedIn for term')
    parser.add_argument('--max', type=int, default=10, help='Max jobs to scrape')
    parser.add_argument('--process', metavar='FILE', help='Process jobs from file')
    parser.add_argument('--all', action='store_true', help='Process all pending jobs')
    parser.add_argument('--list', action='store_true', help='List pending reviews')
    
    args = parser.parse_args()
    
    hunter = JobHunter()
    
    if args.list:
        pending = hunter.tracker.get_pending_review()
        print(f"\nğŸ“‹ Pending Reviews: {len(pending)}")
        for p in pending:
            print(f"  - {p['company']}: {p['title'][:50]}")
            print(f"    Resume: {p.get('resume_path', 'N/A')}")
        return
    
    if args.scrape:
        print(f"ğŸ” Scraping LinkedIn for: {args.scrape}")
        jobs = await scrape_linkedin(args.scrape, args.max)
        if jobs:
            print(f"\nğŸ¤– Now processing with AI scoring...")
            # AIè¯„åˆ†å‡½æ•°ï¼ˆå¤–éƒ¨ä¼ å…¥ï¼Œè¿™é‡Œç”¨æ¨¡æ‹Ÿï¼‰
            async def mock_ai_scorer(job):
                # å®é™…ä½¿ç”¨æ—¶ï¼Œè¿™é‡Œè°ƒç”¨çœŸå®çš„AIè¯„åˆ†
                return 7.5, "AI analysis placeholder"
            
            await hunter.process_jobs(jobs, mock_ai_scorer)
    
    elif args.process:
        print(f"ğŸ“‚ Loading jobs from: {args.process}")
        jobs = load_jobs_from_file(args.process)
        print(f"Found {len(jobs)} jobs")
        
        async def mock_ai_scorer(job):
            return 7.5, "AI analysis placeholder"
        
        await hunter.process_jobs(jobs, mock_ai_scorer)
    
    else:
        parser.print_help()


if __name__ == "__main__":
    asyncio.run(main())
