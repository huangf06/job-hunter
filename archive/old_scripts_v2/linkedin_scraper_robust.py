"""
LinkedIn èŒä½çˆ¬è™« - ç¨³å¥ç‰ˆ
===========================

æ¨¡æ‹Ÿäººå·¥æ“ä½œï¼Œå¢åŠ ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿é¡µé¢å®Œå…¨å“åº”

å…³é”®æ”¹è¿›ï¼š
1. ä½¿ç”¨æ›´é•¿çš„ç­‰å¾…æ—¶é—´
2. é€æ­¥æ»šåŠ¨ï¼Œæ¨¡æ‹Ÿäººå·¥æµè§ˆ
3. æ£€æŸ¥å…ƒç´ æ˜¯å¦å¯è§
4. å¤šæ¬¡é‡è¯•æœºåˆ¶
"""

import asyncio
import json
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional

from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout

PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
COOKIES_FILE = PROJECT_ROOT / "config" / "linkedin_cookies.json"

DATA_DIR.mkdir(parents=True, exist_ok=True)


class LinkedInScraperRobust:
    """LinkedIn èŒä½çˆ¬è™« - ç¨³å¥ç‰ˆ"""
    
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.browser = None
        self.context = None
        self.page = None
    
    async def __aenter__(self):
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=self.headless,
            args=['--disable-blink-features=AutomationControlled']
        )
        self.context = await self.browser.new_context(
            viewport={"width": 1920, "height": 1080},
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )
        self.page = await self.context.new_page()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
    
    async def ensure_login(self):
        """ç¡®ä¿å·²ç™»å½•"""
        print("[LinkedIn] æ£€æŸ¥ç™»å½•çŠ¶æ€...")
        
        if COOKIES_FILE.exists():
            with open(COOKIES_FILE, 'r') as f:
                cookies = json.load(f)
            await self.context.add_cookies(cookies)
        
        await self.page.goto("https://www.linkedin.com/feed/", wait_until="domcontentloaded")
        await asyncio.sleep(3)
        
        if "login" in self.page.url or "signup" in self.page.url:
            print("  â†’ éœ€è¦ç™»å½•ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆ")
            await self.page.goto("https://www.linkedin.com/login")
            input("ç™»å½•å®ŒæˆåæŒ‰å›è½¦...")
            
            cookies = await self.context.cookies()
            COOKIES_FILE.parent.mkdir(parents=True, exist_ok=True)
            with open(COOKIES_FILE, 'w') as f:
                json.dump(cookies, f, indent=2)
            print("  âœ“ Cookies å·²ä¿å­˜")
        else:
            print("  âœ“ å·²ç™»å½•")
    
    async def search_jobs(self, keyword: str, location: str = "Netherlands"):
        """æœç´¢èŒä½ - ç¨³å¥ç‰ˆ"""
        print(f"\n[LinkedIn] æœç´¢: '{keyword}' in {location}")
        print("  ç­›é€‰: Past 24 hours + Hybrid/On-site")
        
        # æ„å»º URL
        base_url = "https://www.linkedin.com/jobs/search"
        params = f"?keywords={keyword.replace(' ', '%20')}"
        params += f"&location={location.replace(' ', '%20')}"
        params += "&f_TPR=r86400"  # Past 24 hours
        params += "&f_WT=2%2C3"     # Hybrid + On-site
        
        url = base_url + params
        print(f"  URL: {url}")
        
        # è®¿é—®é¡µé¢ - ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶
        print("\n  [æ­¥éª¤ 1/4] è®¿é—®é¡µé¢...")
        try:
            await self.page.goto(url, wait_until="networkidle", timeout=90000)
            print("  âœ“ é¡µé¢å·²åŠ è½½")
        except PlaywrightTimeout:
            print("  ! é¡µé¢åŠ è½½è¶…æ—¶ï¼Œä½†ç»§ç»­å°è¯•...")
        
        # ç­‰å¾…åˆå§‹æ¸²æŸ“
        print("\n  [æ­¥éª¤ 2/4] ç­‰å¾…åˆå§‹æ¸²æŸ“ (5ç§’)...")
        await asyncio.sleep(5)
        
        # æ£€æŸ¥èŒä½åˆ—è¡¨æ˜¯å¦å‡ºç°
        print("\n  [æ­¥éª¤ 3/4] æ£€æŸ¥èŒä½åˆ—è¡¨...")
        max_retries = 3
        for attempt in range(max_retries):
            items = await self.page.query_selector_all(".jobs-search-results__list-item")
            print(f"    å°è¯• {attempt + 1}: æ‰¾åˆ° {len(items)} ä¸ªèŒä½å¡ç‰‡")
            
            if len(items) >= 5:
                print(f"  âœ“ èŒä½åˆ—è¡¨å·²å°±ç»ª")
                break
            
            print(f"    ç­‰å¾… 3 ç§’åé‡è¯•...")
            await asyncio.sleep(3)
        
        # å…³é”®ï¼šé€æ­¥æ»šåŠ¨è§¦å‘æ‡’åŠ è½½
        print("\n  [æ­¥éª¤ 4/4] é€æ­¥æ»šåŠ¨è§¦å‘æ‡’åŠ è½½...")
        await self._gradual_scroll()
        
        # æŠ“å–èŒä½
        jobs = await self._extract_jobs()
        return jobs
    
    async def _gradual_scroll(self):
        """é€æ­¥æ»šåŠ¨ï¼Œæ¨¡æ‹Ÿäººå·¥æµè§ˆ"""
        print("    å¼€å§‹æ»šåŠ¨...")
        
        # åˆ† 15 æ¬¡æ»šåŠ¨ï¼Œæ¯æ¬¡æ»šåŠ¨åç­‰å¾…
        for i in range(15):
            # æ»šåŠ¨ä¸€å±
            await self.page.evaluate("window.scrollBy(0, 600)")
            
            # æ¯ 3 æ¬¡æ»šåŠ¨åæ£€æŸ¥èŒä½æ•°é‡
            if (i + 1) % 3 == 0:
                items = await self.page.query_selector_all(".jobs-search-results__list-item")
                print(f"      æ»šåŠ¨ {i + 1}/15: å½“å‰æœ‰ {len(items)} ä¸ªèŒä½å¡ç‰‡")
            
            # ç­‰å¾…æ—¶é—´é€’å¢ï¼Œæ¨¡æ‹Ÿäººå·¥æµè§ˆé€Ÿåº¦
            wait_time = 1.5 + (i * 0.1)
            await asyncio.sleep(wait_time)
        
        # æœ€åå†ç­‰å¾…ä¸€ä¸‹ç¡®ä¿æ‰€æœ‰å†…å®¹åŠ è½½
        print("    ç­‰å¾…æœ€ç»ˆåŠ è½½ (3ç§’)...")
        await asyncio.sleep(3)
        
        # æœ€ç»ˆæ£€æŸ¥
        items = await self.page.query_selector_all(".jobs-search-results__list-item")
        print(f"  âœ“ æ»šåŠ¨å®Œæˆï¼Œå…±æ‰¾åˆ° {len(items)} ä¸ªèŒä½å¡ç‰‡")
    
    async def _extract_jobs(self) -> List[Dict]:
        """æå–èŒä½ä¿¡æ¯"""
        jobs = []
        seen_links = set()
        
        print("\n  å¼€å§‹è§£æèŒä½...")
        
        # ä½¿ç”¨é£å“¥æä¾›çš„å‡†ç¡®é€‰æ‹©å™¨
        items = await self.page.query_selector_all(
            ".jobs-search-results__list-item, li.occludable-update, .job-card-container"
        )
        
        print(f"    å¤„ç† {len(items)} ä¸ªèŒä½å¡ç‰‡...")
        
        success_count = 0
        fail_count = 0
        
        for idx, item in enumerate(items, 1):
            try:
                # æå–èŒä½æ ‡é¢˜
                title_el = await item.query_selector(
                    ".job-card-list__title, .artdeco-entity-lockup__title, .job-card-container__link"
                )
                if not title_el:
                    fail_count += 1
                    continue
                
                title = await title_el.inner_text()
                title = title.strip() if title else ""
                
                # æå–å…¬å¸å
                company_el = await item.query_selector(
                    ".job-card-container__company-name, .artdeco-entity-lockup__subtitle, .job-card-container__company-link"
                )
                company = ""
                if company_el:
                    company = await company_el.inner_text()
                    company = company.strip() if company else ""
                
                # æå–åœ°ç‚¹
                location_el = await item.query_selector(
                    ".job-card-container__metadata-item, .artdeco-entity-lockup__caption"
                )
                location = ""
                if location_el:
                    location = await location_el.inner_text()
                    location = location.strip() if location else ""
                
                # æå–é“¾æ¥
                link = ""
                link_el = await item.query_selector(
                    "a.job-card-list__title, a.artdeco-entity-lockup__title, a.job-card-container__link"
                )
                if link_el:
                    href = await link_el.get_attribute("href")
                    if href:
                        link = href.split('?')[0]
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                if not title or not company or not link:
                    fail_count += 1
                    continue
                
                # å»é‡
                if link in seen_links:
                    continue
                seen_links.add(link)
                
                # è®¡ç®—ä¼˜å…ˆçº§
                priority = "Medium"
                lower_title = title.lower()
                if any(kw in lower_title for kw in ["senior", "lead", "staff", "principal"]):
                    priority = "High"
                if any(kw in lower_title for kw in ["quant", "machine learning", "ai engineer", "ai/ml"]):
                    priority = "High"
                if any(kw in lower_title for kw in ["intern", "junior", "entry"]):
                    priority = "Low"
                
                job = {
                    "title": title,
                    "company": company,
                    "location": location,
                    "url": link,
                    "priority": priority,
                    "source": "LinkedIn",
                    "scraped_at": datetime.now().isoformat()
                }
                jobs.append(job)
                success_count += 1
                
                # æ¯ 5 ä¸ªèŒä½æ‰“å°ä¸€æ¬¡è¿›åº¦
                if success_count % 5 == 0:
                    print(f"    å·²è§£æ {success_count} ä¸ªèŒä½...")
                
            except Exception as e:
                fail_count += 1
                continue
        
        print(f"\n  âœ“ è§£æå®Œæˆ: {success_count} æˆåŠŸ, {fail_count} å¤±è´¥")
        return jobs
    
    def save_jobs(self, jobs: List[Dict], keyword: str):
        """ä¿å­˜èŒä½"""
        date_str = datetime.now().strftime("%Y%m%d_%H%M")
        safe_keyword = re.sub(r'[^\w\-]', '_', keyword.lower())[:20]
        filename = f"linkedin_{safe_keyword}_{date_str}.json"
        filepath = DATA_DIR / filename
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        priority_order = {"High": 0, "Medium": 1, "Low": 2}
        jobs_sorted = sorted(jobs, key=lambda x: priority_order.get(x.get("priority", "Medium"), 1))
        
        data = {
            "source": "LinkedIn",
            "search": keyword,
            "location": "Netherlands",
            "filters": {
                "date_posted": "Past 24 hours",
                "workplace_type": ["Hybrid", "On-site"]
            },
            "scraped_at": datetime.now().isoformat(),
            "total_jobs": len(jobs),
            "jobs": jobs_sorted
        }
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"\n  âœ“ å·²ä¿å­˜ {len(jobs)} ä¸ªèŒä½åˆ°: {filepath}")
        return filepath
    
    def print_summary(self, jobs: List[Dict]):
        """æ‰“å°æ‘˜è¦"""
        print("\n" + "="*70)
        print("æŠ“å–ç»“æœæ‘˜è¦")
        print("="*70)
        
        high_priority = [j for j in jobs if j.get("priority") == "High"]
        medium_priority = [j for j in jobs if j.get("priority") == "Medium"]
        low_priority = [j for j in jobs if j.get("priority") == "Low"]
        
        print(f"\næ€»è®¡: {len(jobs)} ä¸ªèŒä½")
        print(f"  ğŸ”´ é«˜ä¼˜å…ˆçº§: {len(high_priority)} ä¸ª")
        print(f"  ğŸŸ¡ ä¸­ä¼˜å…ˆçº§: {len(medium_priority)} ä¸ª")
        print(f"  ğŸŸ¢ ä½ä¼˜å…ˆçº§: {len(low_priority)} ä¸ª")
        
        if high_priority:
            print(f"\né«˜ä¼˜å…ˆçº§èŒä½:")
            for job in high_priority[:5]:
                print(f"  â€¢ {job['title'][:45]} @ {job['company'][:25]}")


async def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--search", default="data engineer")
    parser.add_argument("--location", default="Netherlands")
    parser.add_argument("--headless", action="store_true")
    args = parser.parse_args()
    
    print("="*70)
    print("LinkedIn èŒä½çˆ¬è™« - ç¨³å¥ç‰ˆ")
    print("="*70)
    print(f"æœç´¢: {args.search}")
    print(f"åœ°ç‚¹: {args.location}")
    print("="*70)
    
    async with LinkedInScraperRobust(headless=args.headless) as scraper:
        await scraper.ensure_login()
        jobs = await scraper.search_jobs(args.search, args.location)
        scraper.save_jobs(jobs, args.search)
        scraper.print_summary(jobs)
        
        print("\n" + "="*70)
        print(f"âœ“ å®Œæˆï¼å…±æŠ“å– {len(jobs)} ä¸ªèŒä½")
        print("="*70)


if __name__ == "__main__":
    asyncio.run(main())
