# =============================================================================
# AI Configuration
# =============================================================================
# Controls AI-powered job analysis and resume generation
# Created: 2026-02-05
# =============================================================================

# =============================================================================
# Model Configuration
# =============================================================================
# API keys and base URLs are loaded from .env file.
# See .env.example for required variables.
# =============================================================================

# Active model for job analysis (switch between "opus" and "kimi")
active_model: "opus"

models:
  # Claude Opus via codesome.cn proxy (Account 1)
  opus:
    provider: "anthropic"
    model: "claude-opus-4-6"
    max_tokens: 8192
    temperature: 0.3
    env_key: "ANTHROPIC_API_KEY"
    env_url: "ANTHROPIC_BASE_URL"
    auth_type: "bearer"  # codesome.cn proxy requires Bearer auth

  # Kimi Coding - uses Anthropic SDK format (messages.create)
  kimi:
    provider: "anthropic"
    model: "k2p5"
    max_tokens: 8192
    temperature: 0.3
    env_key: "KIMI_API_KEY"
    env_url: "KIMI_BASE_URL"

# =============================================================================
# Scoring Thresholds
# =============================================================================

thresholds:
  # Minimum rule-based score to trigger AI analysis
  # Lower threshold = more jobs analyzed = more tokens used
  rule_score_for_ai: 3.0

  # AI score thresholds for actions
  ai_score_generate_resume: 5.0  # >= 5.0: generate tailored resume
  ai_score_apply_now: 7.0        # >= 7.0: mark as priority

# =============================================================================
# Token Budget
# =============================================================================

budget:
  # Daily token limit (approximate)
  daily_limit: 100000
  warn_threshold: 80000

  # Cost tracking (matches configured models)
  cost_per_1k_tokens:
    claude_opus_input: 0.015
    claude_opus_output: 0.075

# =============================================================================
# Prompt Construction Settings
# =============================================================================

prompt_settings:
  # Max chars to include from master resume HTML (actual file ~16k chars)
  master_resume_max_chars: 20000
  # Max chars to include from job description (actual max ~8k chars)
  job_description_max_chars: 10000

  # Work experience sections to include in AI prompt (keys in bullet_library.yaml)
  experience_keys:
    - glp_technology
    - baiquan_investment
    - eleme
    - henan_energy

  # Project sections to include in AI prompt (keys in bullet_library.yaml)
  project_keys:
    - financial_data_lakehouse
    - thesis_uq_rl
    - nlp_projects
    - expedia_recommendation
    - ml4qs
    - deribit_options
    - graphsage_gnn
    - obama_tts
    - lifeos
    - job_hunter_automation

# =============================================================================
# AI Recommendation Thresholds (used in AI prompt)
# =============================================================================

# AI Recommendation Thresholds (used in AI PROMPT — intentionally different from scoring.yaml)
# These tell Claude how to classify ITS OWN scores. Rule-based thresholds are in scoring.yaml.
ai_recommendation_thresholds:
  apply_now: 7
  apply: 5
  maybe: 3

# =============================================================================
# Resume Generation
# =============================================================================

resume:
  # Template settings
  template: "templates/base_template.html"
  output_dir: "output"

  # PDF settings
  pdf:
    format: "A4"
    margin:
      top: "0.55in"
      right: "0.55in"
      bottom: "0.55in"
      left: "0.55in"
    print_background: true

  # Career gap explanation (rendered below experience section)
  career_note: "Career Note: 2019-2023 included independent investing, language learning (English, German), and graduate preparation."

  # Interests line (rendered at bottom of resume)
  interests: "Philosophy (Kant, existentialism), Dostoevsky, powerlifting, analytical writing"

  # Candidate contact info (used by resume renderer)
  candidate:
    name: "Fei Huang"
    email: "huangf06@gmail.com"
    phone: "+31 645 038 614"
    location: "Amsterdam, Netherlands"
    linkedin: "https://www.linkedin.com/in/huangf06/"
    github: "https://github.com/huangf06"
    linkedin_display: "linkedin.com/in/huangf06"
    github_display: "github.com/huangf06"
    blog_url: "https://huangf06.github.io/FeiThink/en/"
    blog_display: "huangf06.github.io/FeiThink/en/"

  # Education details (rendered in template)
  education:
    master:
      school: "Vrije Universiteit Amsterdam"
      location: "Amsterdam, Netherlands"
      degree: "M.Sc. in Artificial Intelligence"
      date: "Sep. 2023 -- Aug. 2025"
      gpa: "8.2/10"
      coursework: "Data Mining (9.0), Deep Learning (9.5), Multi-Agent Systems (9.5), NLP (9.0)"
      thesis: "Uncertainty Quantification in Deep Reinforcement Learning under Noisy Environments"
    bachelor:
      school: "Tsinghua University"
      location: "Beijing, China"
      degree: "B.Eng. in Industrial Engineering"
      date: "Sep. 2006 -- Jul. 2010"
      school_note: "Ranked #1 in China, top 20 globally"
      thesis: "Design and Implementation of Web-based Collaborative Office System (Java/MVC/SQL Server)"
    certification: "Databricks Certified Data Engineer Professional (2026)"

# =============================================================================
# AI Analysis Prompt Template
# =============================================================================

prompts:
  # Main analysis prompt (variables: {bullet_library}, {job_title}, {job_company}, {job_description})
  analyzer: |
    # Job Match Analysis & Resume Tailoring

    You are an expert career advisor helping a job seeker tailor their resume.

    ## Candidate Profile
    Name: Fei Huang
    Education: M.Sc. in Artificial Intelligence, VU Amsterdam (Graduated Aug 2025, GPA 8.2/10)
    Bachelor: B.Eng. Industrial Engineering, Tsinghua University (Ranked #1 in China)

    Key Background:
    - Recent M.Sc. AI graduate with thesis on Uncertainty Quantification in Deep RL
    - Data Scientist & Team Lead at fintech (GLP Technology: credit scoring, PySpark pipelines)
    - Quantitative Researcher at hedge fund (alpha research, live strategy deployment)
    - Built Financial Data Lakehouse portfolio project (Databricks, Spark, AWS, Delta Lake)
    - Databricks Certified Data Engineer Professional (2026)
    - Skills: Python, PyTorch, Spark/PySpark, SQL, Delta Lake, AWS, Docker, Airflow

    ## Bullet Library (Verified Experience)
    {bullet_library}

    ## Target Job
    Title: {job_title}
    Company: {job_company}

    Job Description:
    {job_description}

    ## Your Tasks

    1. **Score the match (0-10)** for these dimensions:
       - skill_match: How well candidate's skills align with requirements
       - experience_fit: How relevant the work experience is
       - growth_potential: Career growth opportunity at this role
       - overall_score: Weighted average considering all factors

    2. **Tailor the resume** by selecting the most relevant content:
       - Bio: write 2-3 sentences if it adds value, or set to null
       - MUST select 2-3 different work experiences from different companies
       - Select 1-3 projects based on JD relevance
       - Select bullets BY THEIR ID (shown in [square brackets] in the library above)
       - Provide 4-6 skill categories from the allowed list
       - Focus on RELEVANCE — include all bullets that match the JD, no artificial limits

    ## Output Format (JSON)

    Respond with ONLY valid JSON (no markdown, no explanation):

    {{
      "scoring": {{
        "overall_score": 7.5,
        "skill_match": 8.0,
        "experience_fit": 7.0,
        "growth_potential": 7.5,
        "recommendation": "APPLY",
        "reasoning": "Strong Python/ML match. PySpark experience aligns with data pipeline requirements."
      }},
      "tailored_resume": {{
        "bio": {{
          "role_title": "Data Engineer",
          "years": 6,
          "domain_claims": ["data_pipelines", "credit_risk"],
          "include_education": true,
          "include_certification": true,
          "closer_id": "eager_company"
        }},
        "experiences": [
          {{
            "company": "GLP Technology",
            "company_note": null,
            "location": "Shanghai, China",
            "title": "Data Scientist & Team Lead",
            "date": "Jul. 2017 -- Aug. 2019",
            "bullets": ["glp_founding_member", "glp_portfolio_monitoring", "glp_pyspark"]
          }},
          {{
            "company": "Baiquan Investment",
            "company_note": null,
            "location": "Beijing, China",
            "title": "Quantitative Researcher",
            "date": "Jul. 2015 -- Jun. 2017",
            "bullets": ["bq_factor_research", "bq_de_factor_engine"]
          }},
          {{
            "company": "Ele.me",
            "company_note": "(acquired by Alibaba)",
            "location": "Shanghai, China",
            "title": "Data Analyst",
            "date": "Sep. 2013 -- Jul. 2015",
            "bullets": ["eleme_ab_testing"]
          }}
        ],
        "projects": [
          {{
            "name": "Financial Data Lakehouse (Databricks/Spark/AWS)",
            "date": "Oct. 2025 -- Present",
            "bullets": ["lakehouse_streaming", "lakehouse_quality"]
          }},
          {{
            "name": "Expedia Hotel Recommendation System",
            "date": "2024",
            "bullets": ["expedia_ltr"]
          }}
        ],
        "skills": [
          {{"category": "Languages & Core", "skills_list": "Python (Expert), SQL (Expert), Bash"}},
          {{"category": "Data Engineering", "skills_list": "PySpark, Delta Lake, Databricks, ETL/ELT, Schema Evolution"}},
          {{"category": "Cloud & DevOps", "skills_list": "AWS (EMR, Glue, S3, Lambda), Docker, CI/CD, Airflow"}},
          {{"category": "Databases", "skills_list": "PostgreSQL, MySQL, Hadoop, Hive"}},
          {{"category": "ML/AI Frameworks", "skills_list": "XGBoost, LightGBM, PyTorch, Statistics, A/B Testing"}}
        ]
      }}
    }}

    ## Important Rules

    ### CONTENT SELECTION PRINCIPLE
    Include ALL content that is relevant to the JD. Exclude content that doesn't strengthen the application.
    The resume can be 1-2 pages — do NOT artificially cut content to fit 1 page.
    Sections order: Education → Projects → Experience → Skills → Interests.
    Education and Interests are static. You control: bio (optional), experiences, projects, skills.

    ### BIO RULES (STRUCTURED FORMAT)
    - Bio is a STRUCTURED OBJECT, not free text. The system assembles the final text.
    - Set bio to null ONLY if no strong angle exists for this role.
    - Select role_title from: {bio_allowed_titles_list}
    - years: integer 4-6 (max 6)
    - domain_claims: pick 1-2 IDs from this list ONLY:
      {bio_domain_claims_list}
    - include_education: true/false (include M.Sc. mention)
    - include_certification: true/false (include Databricks cert)
    - closer_id: "eager_company", "seeking_impact", or null
    - DOMAIN HONESTY: Only select domain_claims relevant to the JD AND supported by actual experience.
    {bio_constraints}

    ### TITLE SELECTION
    {title_context}

    ### COMPANY_NOTE
    - Ele.me: ALWAYS set company_note to "(acquired by Alibaba)"
    - Other companies: set to null

    ### BULLET DISTRIBUTION (GUIDANCE, NOT HARD LIMITS)
    Include bullets based on JD relevance. Typical ranges:
    - Most relevant experience: 2-4 bullets
    - Second experience: 2-3 bullets
    - Third experience (if included): 1-2 bullets
    - Main project: 2-4 bullets (include all that are relevant)
    - Second project: 1-2 bullets
    - Third project (if relevant): 1 bullet

    ### SKILLS FORMAT
    - Provide 4-6 categories from this ALLOWED list ONLY:
      {allowed_skill_categories_list}
    - Do NOT invent category names outside this list
    - Include proficiency levels for Languages (e.g., "Python (Expert)")
    - Include specific cloud services (e.g., "AWS (EMR, Glue, S3, Lambda)")
    - Prioritize skills mentioned in the JD
    - Each category should have 3-6 items

    ### SKILLS HONESTY RULE (ABSOLUTE)
    {skill_context}

    Do NOT:
    - Use "Learning", "Willing to Learn", or "Familiar" qualifiers — these look weak
    - Invent skill categories like "Change Management" or "Infrastructure as Code"
    If the JD requires a skill the candidate lacks, simply omit it — do not fake it.

    ### PROJECT SELECTION
    - Select 1-3 projects based on JD relevance
    - For Data Engineer roles: prioritize Financial Data Lakehouse
    - For ML/DS roles: prioritize Thesis or Expedia
    - For NLP/AI roles: prioritize NLP Projects or Thesis
    - Include a third project only if it adds clearly different skills

    ### BULLET SELECTION RULE (ABSOLUTE)
    Select bullets BY THEIR ID — the string shown in [square brackets] in the library above.
    In the JSON output, put ONLY the bullet ID string, NOT the full text.
    The system will automatically look up the verified text from the library.

    Example: "bullets": ["glp_founding_member", "glp_pyspark"]

    Do NOT:
    - Copy the bullet text into the JSON — use the ID only
    - Invent new bullet IDs that don't exist in the library
    - Modify bullet IDs (use them exactly as shown in square brackets)
    Any unrecognized bullet ID will cause the resume to be REJECTED.

    ### OTHER RULES
    1. Use ONLY bullet IDs from the provided bullet library - do not fabricate
    2. MUST include 2-3 different work experiences from different companies
    3. Select bullets that best match the JD requirements
    4. Include ALL relevant bullets for a position — do not cut for space
    5. Scoring criteria:
       - 8-10: Excellent match, should apply immediately
       - 6-7: Good match, worth applying
       - 4-5: Moderate match, apply if nothing better
       - 1-3: Poor match, likely to be rejected
    6. recommendation values: "APPLY_NOW" (>={apply_now_threshold}), "APPLY" (>={apply_threshold}), "MAYBE" (>={maybe_threshold}), "SKIP" (<{maybe_threshold})

# =============================================================================
# Metadata
# =============================================================================

metadata:
  version: "2.0.0"
  created: "2026-02-05"
  updated: "2026-02-07"
  author: "Fei Huang"
  description: "AI-powered job analysis and resume generation configuration (v2: bullet-by-ID)"
