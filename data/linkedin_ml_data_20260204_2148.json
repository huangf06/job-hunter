{
  "source": "LinkedIn",
  "profile": "ml_data",
  "scraped_at": "2026-02-04T21:48:29.801978",
  "total_jobs": 24,
  "with_jd": 24,
  "jobs": [
    {
      "title": "Data EngineerData Engineer",
      "company": "Ariad",
      "location": "Amsterdam Area (Hybrid)",
      "url": "https://www.linkedin.com/jobs/view/4367165381/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:06.845071",
      "description": "About the job\n\nOur client is a global leader in consumer electronics, known for its cutting-edge innovation and data-driven approach to digital transformation. As data becomes increasingly central to business strategy, this role offers a unique opportunity to shape the future of data infrastructure and analytics within a fast-paced, high-impact environment.\n\nOur client is looking for a technically skilled and collaborative professional to join their team as a Data Engineer. In this role, you will be responsible for building and optimising data pipelines, enabling AI workflows, and supporting data visualisation initiatives across multiple divisions.\n\nYou’ll work closely with another data engineer and collaborate with stakeholders across business, marketing, analytics, and global teams. If you’re passionate about data architecture, automation, and driving innovation, we encourage you to apply for this opportunity.\n\nWhat you will be doing:\n\nDesign and maintain scalable data engineering pipelines using GCP, Azure, and local servers.\nTransform raw data into clean, reusable datasets for business analysts and stakeholders.\nAutomate ETL/ELT processes to ensure data accuracy and relevance.\nPerform data aggregation, filtering, and manipulation to support business needs.\nTroubleshoot and maintain BI tools and guide junior team members.\nManage data warehouse governance and access control.\nDevelop and document technical architecture and security standards.\nSupport dashboard and report creation using BI tools like Power BI.\nCollaborate with stakeholders to align data solutions with business goals.\nPromote data-driven strategies in an accessible and intuitive way across teams.\n\nWhat you bring:\n\nAmbition to further develop your career in data engineering and analytics.\nProven experience in data engineering, data warehousing, and BI (3+ years).\nPast experience in the consumer electronics industry is a plus.\nYour strengths include ETL, data modelling, and cloud computing.\nWorking experience with tools such as Python, SQL, Airflow, dbt, Power BI, and cloud platforms (GCP, Azure, Databricks) is essential.\nStrong stakeholder management and communication skills are needed in this job.\nFluent in English; Dutch is a plus.\nExperience with Git, CI/CD, Linux, and Office tools (Excel, Word, PowerPoint).\nAbility to manage multiple projects and build strong relationships across teams.\nPreferred: experience with RPA, web scraping, real-time pipelines, and GitHub page management.\n\nThe offer:\n\nLong term Contractor collaboration. Mission of 12months with possibilities of extensions.\nDigital recruitment process and onboarding to the company.\nWork for a well-known global brand with a strong presence in the consumer electronics market.\nYou will have the opportunity to work with a collaborative and innovative team focused on data excellence.\nWorking from home: Flexible remote possibilities depending on team needs.",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Machine Learning EngineerMachine Learning Engineer",
      "company": "Group-IB",
      "location": "EMEA (Remote)",
      "url": "https://www.linkedin.com/jobs/view/4359539000/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:08.383327",
      "description": "About the job\n\nWe are looking for an experienced Machine Learning Engineer to strengthen our Fraud Protection team within a multinational cybersecurity company. The primary mission of this role is to analyze banking transactions to detect and prevent fraud and to enhance existing fraud detection algorithms.\n\nA key focus of the position is graph-based transactional analysis, including identifying complex relationships in financial data such as cryptocurrency flows. You may also contribute to broader graph-related tasks, for example building GraphRAG components for AI-driven agents.\n\nTech Stack\n\nPython 3.8+\nData analysis & modeling: pandas, scikit-learn, PyTorch\nGraph libraries/frameworks: NetworkX, PyG, Neo4j, AWS Neptune (any of these)\nDatabases (nice to have): Elasticsearch, Cassandra, ClickHouse\nGit, Docker\nAWS ecosystem\n\nResponsibilities\n\nIdentify and formalize scenarios to be covered by the transactional antifraud system, especially in the AML domain.\nConduct research on cutting-edge ML models and techniques for fraud and anomaly detection in transactional data.\nBuild ML models (classic, graph-based, neural networks) and integrate them into the Fraud Protection platform.\nDevelop production-ready source code (primarily in Python; Go experience is a plus).\nDocument and present research findings.\n\nThis role is perfect for you if you:\n\nHave a solid foundation in transactional data analysis.\nUnderstand neural networks (CNNs, RNNs, etc.), classical ML algorithms, and where to apply them—especially Graph ML (node embeddings, link prediction, GNNs).\nHave hands-on experience with frameworks such as PyTorch or Keras/TensorFlow.\nAre proficient in both IDE-based and terminal-based development.\nHave experience designing and building ETL pipelines.\nHave a research-oriented mindset and enjoy experimentation.\n\nPreferred:\n\nStrong documentation and presentation skills.\nExperience with MLOps practices.\nExperience writing production-ready asynchronous Python code.\n\nWhy choose Group-IB\n\nYour happiness is important to us. We want every single team member to be happy.\nContinuing professional development. At Group-IB, you can choose from various paths to growth: progress as an expert, advance to a management position, try your hand in another department, relocate abroad, or launch a new business area at Group-IB.\nA team with extensive international expertise. Do you have experience but are looking for exciting challenges? By choosing us, you will be choosing complex tasks and continuously improving your skills in a fast-growing international company.\nGlobally recognized technologies. Group-IB's offices are located in seven countries and our products and services are sold in 60 countries. What’s more, Gartner, IDC, and Forrester have ranked our technologies among the best in their class. We work with over 450 international partners and about 500 clients.\nA culture created by each of us. Group-IB’s employees speak many different languages and understand one another. We respect each other's beliefs, share common values, and strive toward the happiness of every employee.\nEconomic stability. Group-IB's sustainable growth helps rapidly develop careers that would take years to progress as far as most other companies.\n\nWhat else you should know\n\nHealth. If anything goes wrong, don’t worry — we offer health insurance.\nCertificates and training courses. Group-IB specialists hold over 1,000 professional certificates, including CEH, CISSP, OSCP, GIAC, MCFE, BSI, as well as some rare ones that would be a source of pride for experts in forensics, penetration testing, and reverse engineering worldwide. We have an incentive program that helps employees achieve certifications at the company's expense.\nChallenges. A wide selection of GIB programs helps you improve soft skills, gain new competencies, and receive monetary rewards.\nThe initiative is rewarded. At Group-IB, you can bring your most daring ideas to life. The company encourages technical blogging, writing articles, building sports teams, and other creative activities.\n\nSounds like you? Apply now!",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Senior Data Engineer",
      "company": "Team EIFFEL",
      "location": "Arnhem, Gelderland, Netherlands (Hybrid)",
      "url": "https://www.linkedin.com/jobs/view/4368766479/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:08.566230",
      "description": "About the job\n\nAls Data Engineer bij EIFFEL verbind je systemen en mensen. Je bouwt bruggen tussen complexe data en waardevolle inzichten. Bij ons bepaal jij hoe je jezelf ontwikkelt - of je nu specialist wordt in één domein of verschillende technieken verkent.\n\nDit ga je doen\n\nData stroomt door elke organisatie. Jij zorgt dat deze data op de juiste plek landt en bruikbaar wordt voor de business. Je werkt aan uitdagende projecten bij grote organisaties zoals de Rabobank, waar je datawarehouse-omgevingen ontwikkelt.\n\nJe verzamelt en beheert data uit verschillende bronnen zoals databases, logbestanden en externe API's;\nJe werkt met moderne tools en technieken zoals SQL, Python, Cloudplatforms Azure/AWS, Snowflake;\nJe ontsluit databronnen voor eindgebruikers zoals in uiteenlopende sectoren;\nJe transformeert en modelleert data zodat het direct toepasbaar is voor BI-analyses;\nJe gebruikt jouw basiskennis van BI-tools voor een optimale samenwerking met analisten;\nJe hebt bij voorkeur kennis van dimensioneel modelleren;\nJe bouwt mee aan onze dienstverlening en begeleidt actief junior en medior consultants.\n\nJe bent fit for the job als je...\n\nEen ervaren professional bent die verder kijkt dan de techniek. Je ziet kansen voor verbetering en pakt ze met beide handen aan.\n\nMinimaal 5 jaar werkervaring in een soortgelijke functie hebt;\nMinimaal een afgeronde hbo-opleiding richting Business IT & Management of vergelijkbaar hebt;\nRuime ervaring met ETL/ELT zoals Data factory, DBT, Airflow en Postman hebt;\nErvaring met (Restful) API developmen en data integraties hebt;\nErvaring met CI/CD en versiebeheer via GIT hebt;\nErvaring hebt met AVG-wetgeving en begrijpt hoe data hierin opgezet moet worden;\nIn het bezit bent van een rijbewijs B;\nBeschikt over communicatieve vaardigheden in het Nederlands, zowel mondeling als schriftelijk.\nFor English speaking candidates: our consultants need to be able to communicate in Dutch at a minimum of C1 level, spoken and written.\n\nDit krijg je van ons\n\nWe bieden een salaris tussen de €4.000,- en €7.000,- bruto per maand op basis van 40 uur per week passend bij jouw ervaring en expertise.\n\nDit zijn wij\n\nGroot in interim-capaciteit, steengoed in consultancy. We versterken organisaties precies waar het nodig is, of het nu gaat om een bank, gemeente of ziekenhuis. Door slimmer te werken. Door ideeën van papier naar praktijk te brengen. En door de juiste mensen op de juiste plek te zetten. Voor grote transformaties én snelle oplossingen. Van multinational tot start-up, als professional bij EIFFEL werk je overal.\n\nJe wordt onderdeel van Team Analytics, een groeiend team van gedreven professionals. Samen zetten jullie datagedreven werken centraal bij verschillende organisaties door heel Nederland. Bij EIFFEL wordt op vrijdagen een samenwerkdag gehanteerd in ons clubhuis in Arnhem. Deze staan in het teken van kennisdeling met collega's in het werkveld en binding houden met Team EIFFEL als werkgever. Kanttekening is dat wanneer de opdrachtgever jou op vrijdag verwacht bij hen op kantoor, dit uiteraard voor gaat.\n\nEIFFEL is onderdeel van Team EIFFEL, een community van 3000 experts in de wereld van interim, consultancy en projectmanagement. Met opdrachten in 24 kennisdomeinen bij 600 opdrachtgevers en meer trainingen dan je je kunt voorstellen, helpen onze Performance Managers jou een carrièrepad uit te stippelen dat zich naar jouw ambitie en levensfase vormt.\n\nDeze Skills Komen Van Pas\n\nSQL\nRESTful API development\nKennis van programma's en portfolio's\nKennis van BI-tools\nETL processen\n\nJe combineert technische expertise met sterke communicatieve vaardigheden. Je schakelt makkelijk tussen verschillende niveaus - van technisch specialist tot eindgebruiker. Je bent nieuwsgierig naar nieuwe ontwikkelingen en deelt graag kennis met collega's.",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Senior Data EngineerSenior Data Engineer",
      "company": "Near.U",
      "location": "European Union (Remote)",
      "url": "https://www.linkedin.com/jobs/view/4368757575/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:08.680355",
      "description": "About the job\n\nAbout Us\n\nWe are Near.U, We strive to develop our solutions by observing the best practices of our craft this means enabling our company to iterate our products fast and stay ahead. We are looking to grow our Team with a Senior Data Engineer.\n\nAbout the Role\n\nWe are looking for a Data Engineer to manage and evolve a Enterprise Data platform.\n\nYou will play a key role in technical operations, data platform evolution, and enterprise data projects, working closely with business and IT stakeholders in an agile environment.\n\nThis role combines operational responsibility with design, advisory, and continuous improvement activities on the Data platform.\n\nBecause we work with various customers, you are also expected to be able to adapt to different environments. Near.U is an international trademark of MoOngy with a very inclusive culture. We are looking for people that share those values with us and can work well across teams and individual roles.\n\nWhat You Bring To The Position\n\nProven experience as a Data Engineer in enterprise environments\nStrong experience with the Microsoft Data stack (e.g. Data Lake, Azure Data services, Power BI)\nHands-on experience with ETL/ELT pipelines, data transformation, and data modeling\nExperience operating and supporting production data platforms\nUnderstanding of data security, governance, and compliance principles\nExperience working in agile delivery models\nAbility to work autonomously while collaborating with distributed teams\nGood level of English.\n\nQualifications\n\nDegree in Computer Engineering or similar\n\nKey Responsibilities\n\nMonitor, maintain, and troubleshoot the Data environment using alerts and monitoring tools\nAct as an expert for problem resolution and platform enhancements during ongoing operations\nSupport and maintain the Data environment, including:\n\nETL / ELT pipelines\nData staging and transformation\nPower BI datasets and reports\nExcel-based sources migrating to a cloud-based data mart\n\nContribute to the continuous improvement of the Data platform and data engineering best practices\nParticipate in the selection, design, and development of the Data Lake and integrated application architecture\nDefine and improve interfaces between source systems and the Data platform\nProvide design, exploration, and advisory support on data initiatives\nParticipate in enterprise Data projects as part of a data product team\nDeploy new Data products in an agile manner, contributing to backlog refinement, planning, and prioritization\nEnsure delivery aligns with business priorities and capacity constraints\nContribute to platform governance, administration, and change management\nEnsure a secure-by-design approach, staying up to date with data security standards and regulations\nEnsure architecture and development comply with internal and regulatory requirements\nDesign and implement automation and monitoring capabilities to improve operational performance\nEnsure reliable, secure, and efficient data operations supporting business and employee needs\nUndertake additional tasks assigned by management, with autonomy on the technical scope\nActively contribute to knowledge sharing and best practices within the data community\n\nPlease note: Our Engineering team is predominantly based in Portugal, nevertheless this position is currently also open to residents in EU.\n\nMoOngy Group is an Equal Opportunity employer. All applicants will be considered without regard for race, colour, national origin, ethnicity, gender, disability, sexual orientation, gender identity, or religion.",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Computer Vision Engineer - MLOpsComputer Vision Engineer - MLOps",
      "company": "DeepRec.ai",
      "location": "European Union (Remote)",
      "url": "https://www.linkedin.com/jobs/view/4369218686/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:08.754736",
      "description": "About the job\n\nComputer Vision Engineer - MLOps\n\nLocation: Remote - (Must be EU based)\n\nSalary: Base salary + equity\n\nWe are hiring a Computer Vision Engineer to join a robotics company building intelligent, flexible robots for real manufacturing environments. This role is focused on production systems. You will be working on perception pipelines that run 24/7 on deployed robotic cells.\n\nThe robots follow a modular architecture where new capabilities are continuously added, such as object recognition, grasp point estimation, anomaly detection, and task specific behaviours. As the system scales, the ML and MLOps foundations become critical. This role exists to own and extend those foundations.\n\nWhat you will be doing\n\nDesign, build, and deploy 2D and 3D computer vision systems used in live production environments. This includes image classification, object detection, semantic and instance segmentation, metric learning, and smart filtering.\nTake models end to end, from data and training through to deployment, optimisation, and monitoring.\nContribute directly to an in house MLOps platform that supports data ingestion, experiment tracking, model versioning, deployment, and observability across multiple robotic capabilities.\nWork closely with robotics and hardware focused teams and help ensure models run efficiently and reliably on edge and production hardware. Over time, this includes model conversion and optimisation using tools such as ONNX and TensorRT.\n\nWhat we are looking for\n\nThis is a production engineering role. We are looking for someone who has built ML systems before.\n\nNon-negotiable experience:\n\nHands-on experience working with visual data in production systems (2D and/or 3D computer vision).\nProven production ML experience: you have taken models from training through deployment and supported them in live environments.\nStrong Linux fundamentals, including working over SSH and operating production infrastructure.\nYou have built MLOps systems, not just used them. This includes ownership of data pipelines, experiment tracking, model versioning, deployment, and monitoring.\nSolid understanding of how models actually work under the hood. You are comfortable reasoning about backpropagation, gradients, network architectures, and debugging model behaviour when things go wrong.\n\nNice to have:\n\nExperience with robotics, autonomous systems, or other edge-deployed ML.\nSynthetic data generation and the ability to design efficient data collection strategies.\nModel conversion and optimisation workflows using ONNX and/or TensorRT.\nExperience with ROS, Kubernetes, and cloud platforms.\n\nWhy apply?\n\nThis role offers a rare combination of real-world impact and deep technical ownership. You are not optimising isolated models or working on disconnected experiments. You are helping define the perception and MLOps foundations for intelligent robotic systems that are already operating in production and will continue to scale over time.\n\nEngineers work in pods with clear ownership and the opportunity to grow into leading entire problem areas. Progression and compensation are tied to impact rather than tenure. The environment is fast-moving and flexible, with intense periods of work when needed, and a strong emphasis on transparency and alignment.\n\nThis is a role for someone who wants to see their work move quickly from code to real machines on real factory floors.",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Senior Machine Learning Scientist I - GenAI",
      "company": "Booking.com",
      "location": "Amsterdam, North Holland, Netherlands (Hybrid)",
      "url": "https://www.linkedin.com/jobs/view/4361647689/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:08.821525",
      "description": "About the job\n\nAt Booking.com, data drives our decisions. Technology is at our core and innovation is everywhere. But our company is more than datasets, lines of code or A/B tests. We’re the thrill of the first night in a new place. The excitement of the next morning. The friends you make. The journeys you take. The sights you see. And the food you sample. Through our products, partners and people, we can empower everyone to experience the world.\n\nAbout the team:\n\nThis opening is for the GenAI Applications Team within the Data & AI Marketplace department.\n\nThe GenAI Applications team is responsible for designing and delivering agentic, ML-powered solutions for some of our most impactful products, including booking search experiences, trip planning, and trip helpfulness. The team builds AI-driven applications and conversational agents, such as chatbots and intelligent assistants, that significantly enhance the end-to-end customer experience.\n\nAs a Senior Machine Learning Scientist, you will work closely with engineers and to design, develop, and evaluate machine learning solutions for scalable, customer-facing GenAI applications. Your work will focus on researching, training, fine-tuning, and rigorously evaluating models leveraging LLMs, recommendation systems, and agent-based architectures, using state-of-the-art techniques. You will drive experimentation, define success metrics, and translate insights into impactful AI solutions that shape the future of intelligent travel products.\n\nKey Job Responsibilities and Duties:\n\nExplore and apply state-of-the-art techniques in multimodal machine learning.\nTrain innovative ML models (NLP, CV, LLM-finetuning…), build algorithms, and engineering approaches to drive business impact..\nCoding skills: ensure implementation of reusable frameworks (clean and scalable code).\nConduct data analysis with detailed metrics to evaluate model’s performance, labels quality, features exploration.\nWork closely with machine learning engineers to ensure the model's latency/throughput meets product requirements and ensure deployment of your model to production.\nCollaborate with multidisciplinary teams: Collaborate with product managers, data scientists, and analysts to understand business requirements and translate them into machine learning solutions.\n\nQualifications & Skills:\n\nAdvanced knowledge and experience in Computer Vision and Natural Language Processing, engineering aspects of developing ML and GenerativeAI models at scale.\nExperience designing and executing end-to-end research and development plans and generating impact through large-scale machine learning model development. Preferably evidenced by peer-reviewed publication, patents, open sourced code or the like.\nRelevant work or academic experience (MSc + 6 years of working experience, or PhD + 4 years of working experience), involved in the application of Machine Learning to business problems.\nMasters degree, PhD or equivalent experience in a quantitative field (e.g. Computer Science, Engineering Mathematics, Artificial Intelligence, Physics, etc.).\nExperience on multiple machine learning facets: working with large data sets, model development, statistics, experimentation, data visualization, optimization, software development.\nExperience collaborating cross functionally in the development of machine learning products (e.g. Developers, UX specialists, Product Managers, etc.).\nStrong working knowledge of Python, Java, Kafka, Hadoop, SQL, and Spark or similar technologies. Working experience with version control systems.\nExcellent English communication skills, both written and verbal.\nSuccessfully driving technical, business and people related initiatives that improve productivity, performance and quality while communicating with stakeholders at all levels\nLeading by example, gaining respect through actions, not your title. Developing your team and motivating them to achieve their goals. Providing feedback timely and managing your key team performance indicators\n\nBenefits & Perks - Global Impact, Personal Relevance:\n\nBooking.com’s Total Rewards Philosophy is not only about compensation but also about benefits. We offer a competitive compensation and benefits package, as well unique-to-Booking.com benefits which include:\n\nAnnual paid time off and generous paid leave scheme including: parent, grandparent, bereavement, and care leave\nHybrid working including flexible working arrangements, and up to 20 days per year working from abroad (home country)\nIndustry leading product discounts - up to 1400 per year - for yourself, including automatic Genius Level 3 status and Booking.com wallet credit\n\nInclusion at Booking.com:\n\nInclusion has been a core part of our company culture since day one. This ongoing journey starts with our very own employees, who represent over 140 nationalities and a wide range of ethnic and social backgrounds, genders and sexual orientations.\n\nTake it from our Chief People Officer, Paulo Pisano: “At Booking.com",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Machine Learning EngineerMachine Learning Engineer",
      "company": "Deepdesk",
      "location": "Rotterdam, South Holland, Netherlands (Hybrid)",
      "url": "https://www.linkedin.com/jobs/view/4358897102/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:08.890211",
      "description": "About the job\n\nAbout Deepdesk\n\nDeepdesk builds real-time AI assistance that empowers customer service agents across chat, email, and voice. Our Agent Assist solution leverages cutting-edge machine learning, NLP, and voice technologies to deliver instant suggestions, rewriting, context understanding, and emerging agentic AI capabilities.\n\nWe are shaping the next generation of intelligent customer support through fast, production-grade systems that work seamlessly across multilingual and omnichannel environments.\n\nRole Overview\n\nWe’re looking for a Machine Learning Engineer to develop and optimise the ML intelligence behind Deepdesk’s Agent Assist platform. This role is perfect for someone who thrives in fast-paced, production environments and wants to work on impactful, real-time AI systems.\n\nYou’ll design and deploy ML/NLP models, enhance voice/STT pipelines, and collaborate closely with engineering to push the boundaries of agentic AI. Your work will directly improve how customer service agents communicate across chat, email, and voice.\n\nKey Responsibilities\n\nBuild and optimise ML and NLP models for real-time agent assist.\nDevelop algorithms for search, autocomplete, ranking, and rewriting.\nImplement and refine multilingual and omnichannel (text + voice) capabilities.\nIntegrate, tune, and deploy Speech-to-Text (STT) pipelines for voice-based use cases.\nRun experiments, model evaluations, and performance tuning.\nDesign scalable ML infrastructure, monitoring, and production-ready components.\nWork closely with engineering to shape new agentic AI workflows.\n\nMust-haves\n\nStrong Python development and algorithmic skills.\nExperience in ML or NLP (e.g., embeddings, classification, transformers).\nHands-on expertise with PyTorch or TensorFlow.\nAbility to build clean, reliable, production-grade ML components.\n\nNice-to-haves\n\nExperience with voice/STT models (Whisper, wav2vec2, DeepSpeech, etc.).\nBackground in agent assist, conversational AI, or agentic AI systems.\nFamiliarity with Kubeflow, MLOps, or Google Cloud Platform (GCP).\n\nTech Stack\n\nPython · TensorFlow · PyTorch · Scikit-learn · SpaCy · Kubeflow · GCP · Whisper / wav2vec2",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "AI Engineer",
      "company": "Solvinity",
      "location": "Amersfoort, Utrecht, Netherlands (Hybrid)",
      "url": "https://www.linkedin.com/jobs/view/4358887291/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:12.020444",
      "description": "About the job\n\nAI is geen hype meer, maar een gamechanger. Bij Solvinity gaan we AI inzetten waar het écht verschil maakt: processen automatiseren, engineers ontlasten, en complexe vragen beantwoorden. Jij bent de engineer die dit mogelijk maakt. Je bouwt slimme integraties tussen ServiceNow, GitLab, security tooling en onze infrastructuur. Je creëert AI-agents die niet alleen taken uitvoeren, maar ook de context begrijpen.\n\nAls AI Engineer bij Solvinity ben je de pionier van een compleet nieuw vakgebied binnen ons bedrijf. Het team van engineers wordt door onze klanten geprezen, en jij gaat ervoor zorgen dat ze nóg effectiever hun werk kunnen doen. Dit doe je door slimme AI-oplossingen te bouwen die informatie uit verschillende systemen combineren en daarmee bruikbare inzichten geven en taken automatiseren.\n\nJe hebt een passie voor AI, maar je bent vooral pragmatisch. Je weet dat de mooiste AI-oplossing waardeloos is als hij niet betrouwbaar werkt of moeilijk te onderhouden is. Je combineert kennis van LLMs, prompt engineering, RAG-architecturen en MCP-servers met stevige development skills in Python. API-integraties en data-pipelines bouwen is voor jou second nature.\n\nJe pioniert graag, maar verliest niet uit het oog dat jouw oplossingen straks door anderen gebruikt en onderhouden moeten worden. Je denkt in producten, niet in proof-of-concepts. De vraag is niet óf je AI wilt toepassen, maar hoe je dat op een verantwoorde en effectieve manier doet. Je skillset ga je zeker nog verder ontwikkelen, want als Solvineer kun je dat!\n\nDe dag van een AI Engineer bij Solvinity is gevuld met uitdagende projecten: een mooie mix van onderzoek, ontwikkeling en integratie. Je bouwt AI-oplossingen die ons bedrijf slimmer maken:\n\nJe ontwikkelt slimme call routing en escalatie-mechanismen\nJe bouwt een AI-assistent die generieke vragen beantwoordt en aan onze knowledge base koppelt\nJe zorgt dat calls compleet zijn: de AI vraagt door als essentiële informatie ontbreekt\nJe ontwikkelt een contract agent die contractdetails doorzoekbaar maakt\n\"Welke contracten lopen de komende 6 maanden af?\" wordt een simpele vraag\nJe integreert AI met GitLab, Wireshark/pcap data en monitoring tooling\nDoor middel van integratie worden complexe vragen opeens snel beantwoord\n\nWat verwachten we van jou?\n\nSterke kennis van Python en moderne AI/ML frameworks (LangChain, OpenAI API, etc.)\nErvaring met LLMs, prompt engineering en RAG-architecturen\nBekend met API-integraties en het bouwen van data pipelines\nKennis van vector databases en semantic search\nErvaring met Git, CI/CD en containerization (Docker/Kubernetes)\nJe snapt hoe je AI-modellen evalueert en monitort\nJe denkt in producten en oplossingen, niet alleen in technologie\nJe bent pragmatisch: de beste oplossing is degene die werkt én onderhoudbaar is\nJe bent bewust van privacy, security en AI-ethics\nUitstekende Nederlandse en Engelse spreek- en schrijfvaardigheid\n\nWat krijg je:\n\n€ 3.822 - € 5997,- bruto p/m (afhankelijk van kennis en ervaring);\nBoeiend en uitdagend werk in een professionele werkomgeving waar collegialiteit en samenwerken de sfeer bepalen, en je je werkweek flexibel kunt invullen;\nSolvinity investeert in jouw ontwikkeling en het najagen van jouw passies met opleidingen en trainingen en geeft je de ruimte om je talenten te ontplooien;\nEen cultuur die innovatie, samenwerking en continue ontwikkeling bevordert;\n32 tot 40-urige werkweek;\nReiskostenvergoeding of thuiswerkvergoeding\nHybride werken vanuit huis en ons kantoor in Amersfoort\n80% pensioenbijdrage werkgever;\n26 vakantiedagen met de mogelijkheid tot uitbreiding;\nEen vitaliteitsregeling waarbij we tot 300 euro per jaar jouw sportactiviteiten financieren.\n\nGroeipad:\n\nSolvinity biedt ruime ontwikkelmogelijkheden aan, zoals functiegerelateerde basis- en loopbaanopleidingen. Na twee jaar in dienst heb je €1000 extra aan opleidingsbudget dat je zelf mag spenderen. Je kunt hierbij denken aan seminars, coaching, NLP of inhoudelijke cursussen, zo lang het maar in het bedrijfsbelang is. Jaarlijks bespreek je jouw persoonlijke opleidingsplan.\n\nWat zijn Solvineers?\n\nOnze mensen zijn IT-specialisten met een nieuwsgierige, kritische, slimme en coachende instelling. Maar een Solvineer heeft ook soft skills als empathie en teamgeest: we zijn allround teamspelers en hechte collega's. \"Nooit stoppen met leren\" is ons motto, daarom bieden we Solvineers trainingen, meet-ups en evenementen om op de hoogte te blijven en nieuwe vaardigheden te leren.\n\nHoe bereik je ons?\n\nPassen we bij elkaar? Wil je ons daarvan overtuigen? Reageer dan via de sollicitatiebutton of mail met Cynthia, onze Recruiter via recruitment@solvinity.com.\n\nWerken bij Solvinity\n\nSolvinity biedt Secure Managed Cloud Services, met name gericht op organisaties met hoge beveiligingseisen, waaronder de (rijks-)overheid, gemeenten en vooraanstaande financiële en zakelijke dienstverleners. We onderscheiden ons door innovatieve managed cloud oplossingen, cybersecurity-expertise en een ui",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Data engineerData engineer",
      "company": "Alpina Group",
      "location": "Leusden, Utrecht, Netherlands (Hybrid)",
      "url": "https://www.linkedin.com/jobs/view/4325187225/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:12.100062",
      "description": "About the job\n\nBen jij een ervaren data engineer die graag de toekomst van data binnen een groeiende organisatie vormgeeft? Jij speelt een sleutelrol bij de overgang van onze on-prem BI-omgevingen naar een moderne Azure Lakehouse-architectuur. Met jouw expertise zorg je voor stabiele dashboards en rapportages, terwijl je samen met collega’s de datastrategie van morgen bouwt.\n\nBij ons draait het niet alleen om wat je kunt, maar ook om wie je bent. Dus: ben jij die proactieve Data Engineer die met ons het verschil maakt?\n\nJouw rol in het team\n\nAls data engineer bij Alpina werk je aan de stabiliteit van onze bestaande Microsoft BI-omgeving én draag je bij aan de migratie naar Azure Lakehouse. Jij zorgt ervoor dat data betrouwbaar en beschikbaar is voor strategische inzichten.\n\nJij beheert en optimaliseert onze huidige on-prem Microsoft BI-omgevingen.\nJij ontwikkelt, onderhoudt en deployt SSIS-pakketten voor ETL-processen.\nJij werkt met SSRS en SSAS (Tabular en Multidimensional) en voert performance tuning uit.\nJij onderhoudt dimensionele modellen zoals star schema’s.\nJij ondersteunt de migratie naar een Azure Lakehouse omgeving en denkt mee over versiebeheer en CI/CD-trajecten.\n\nWat wij jou bieden\n\nEen werkweek van 32 tot 40 uur.\nDirect een arbeidsovereenkomst voor onbepaalde tijd.\nEen bruto maandsalaris van €5.021,- tot €6.275,- op basis van 40 uur.\nMogelijkheid tot hybride werken met als standplaats Leusden.\nEen leasefietsregeling.\nEen reiskostenvergoeding van € 0,21 per km met de auto, € 0,25 per km met de fiets of te voet, en reizen met het OV in de 2e klas wordt volledig vergoed.\n27 vakantiedagen als je 40 uur werkt.\nPersoonlijk Keuzebudget (PKB): Jij bepaalt zelf hoe je een deel van je arbeidsvoorwaarden inzet: meer verlof, extra salaris of een opleiding.\nAlpina Academy: Toegang tot trainingen, cursussen en opleidingen, o.a. op het gebied van vakkennis, AI, persoonlijke groei of leiderschap.\nKorting op financiële producten: Denk aan verzekeringen, hypotheken, taxaties en aan- of verkoop van je woning.\n\nMaak kennis met Alpina Group\n\nJij komt te werken binnen Alpina Group, een landelijk werkende organisatie met sterke regionale wortels. Je werkt in een dynamisch team van 16 collega’s verspreid over meerdere kantoren, met één maandelijkse samenkomst in Leusden. Het team is informeel, betrokken en innovatief, met ruimte voor eigen initiatief en verbetervoorstellen.\n\nMet 1.500 collega’s op ruim 40 locaties zorgt Alpina group dat klanten volledig worden ontzorgd in financiële dienstverlening. Geen stropdassenmentaliteit, maar een informele sfeer waarin ondernemerschap en proactiviteit belangrijker zijn dan hiërarchie. En met een doel: dat jij hier alles mag worden behalve ongelukkig.\n\nWat jij in huis hebt\n\nMinimaal 5 jaar relevante werkervaring in data engineering.\nAantoonbare ervaring met Microsoft SQL Server en het schrijven van geoptimaliseerde T-SQL queries.\nRuime ervaring met SSIS, SSRS en SSAS (Tabular/Multidimensional).\nGrondige kennis van datawarehousing en dimensioneel modelleren.\nProactieve houding en in staat om zelfstandig én in teamverband te werken aan dataoplossingen.\nGoede sociale vaardigheden en een uitstekende beheersing van de Nederlandse taal in woord en geschrift.\n\nWat we als pluspunt zien\n\nErvaring met migratie van SSRS-pakketten richting de cloud.\nErvaring met Git of andere versiebeheersystemen; kennis van CI/CD-trajecten.\nErvaring in en kennis van Azure DevOps.\nKennis van Databricks, DBT of Azure Data Factory.\n\nDurf jij het aan?\n\nSolliciteer vandaag nog of neem contact op met Demi Kemperman via telefoonnummer 06-51057153 voor meer informatie.\n\nAcquisitie naar aanleiding van deze advertentie wordt niet op prijs gesteld.",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Principal AI EngineerPrincipal AI Engineer",
      "company": "Infogain Poland",
      "location": "European Union (Remote)",
      "url": "https://www.linkedin.com/jobs/view/4358035249/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:12.173484",
      "description": "About the job\n\nProject Info\n\nWe are building a proprietary, industrial-grade AI platform to tackle the \"last mile\" of Generative AI: guaranteeing correctness in complex domains.\n\nWe are not looking for a \"Prompt Engineer\" or an API integrator. We are seeking a Research-Grade Engineer (Masters/PhD preferred) who combines deep theoretical knowledge of NLP with the ability to architect scalable, production-ready systems. You will function as the technical lead of this platform, working directly with the CTO to define the next generation of code intelligence.\n\nCurrent LLMs are excellent at creativity but struggle with the strict logical consistency required for enterprise software. Your challenge is to bridge this gap. You will design systems that can handle massive context windows, maintain semantic integrity across thousands of files, and deliver verifiable accuracy.\n\nYou will define the methodologies to constrain Generative AI with strict structural rules, answering the hard question: How do we build a system that possesses the flexibility of a neural network but the reliability of a compiler?\n\nResponsibilities\n\nArchitecture Design: Architect high-reliability inference systems that solve the \"hallucination problem\" inherent in Large Language Models. You will move beyond out-of-the-box solutions to build defensible, proprietary IP.\nAdvanced NLP Strategy: Define the strategy for domain adaptation and long-context reasoning. You will perform first-principles analysis to select the right approach (RAG, Fine-Tuning, or novel methods) based on rigorous benchmarking.\nEvaluation & Verification: Design and build proprietary evaluation frameworks to rigorously measure the performance and safety of our models before they touch client code.\nTechnical Standards: Mentor the engineering team on the mathematical underpinnings of Transformer architectures and current SOTA research.\n\nJob Requirements\n\nAdvanced Degree: Masters or PhD in Computer Science, AI, or related field (or equivalent top-tier research lab experience).\nAdvanced LLM Internals: You understand the specific failure modes of modern architectures regarding long-context recall, reasoning drift, and hallucination triggers in complex logic. You don't just fine-tune; you know how to mathematically constrain model outputs to ensure high-fidelity results.\nApplied Research: 8+ years of experience, with a track record of taking complex ML research and deploying it into production environments.\nBeyond APIs: Proven expertise in architecting autonomous agentic systems that go beyond simple retrieval. You have designed multi-agent orchestrators involving planning, tool use (function calling), and self-correction loops to solve multi-step reasoning problems reliably.\nEngineering Excellence: Strong proficiency in Python. You write clean, modular, object-oriented code, not just \"notebook scripts.\"\n\nPreferred Experience\n\nInterest in Code Generation, Program Analysis, or Semantic Parsing.\nExperience with open-source LLM orchestration (LangChain, DSPy, LlamaIndex) but with a critical understanding of their limitations.\nPublished research or technical blog posts on Applied NLP\n\nBenefits\n\nGeneral benefits - depends on the form of employment\n\nHybrid work model combining office & remote work\nAttractively located office with collaboration spaces\nOnsite parking space for employees\nReferral program with financial bonus\nLife Insurance\nBudget for development (including language courses and others), clear career path with the possibility to gain experience in international environment\nAccess to internal Learning Platform with multiple trainings oriented for professional growth\n\nLifestyle benefits:\n\nAccess to MyBenefit platform (Multisport included)\nTeam Building activities\nCharity initiatives\nWorking environment promoting diversity and inclusion\n\nHealth benefits:\n\nPrivate medical care - Platinum Package",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Lead Machine Learning Operations",
      "company": "Stedin",
      "location": "Rotterdam, South Holland, Netherlands (Hybrid)",
      "url": "https://www.linkedin.com/jobs/view/4369093876/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:12.234912",
      "description": "About the job\n\nDeze vacature is ingeschaald in schaal C10.\n\nJouw energie in de plus\n\nIn deze rol bepaal jij de technische koers van ons ML/AI platform en zorg je dat onze machine learning producten betrouwbaar, schaalbaar en toekomstbestendig zijn.\n\nWaarom Werken Bij Stedin Jou Energie Oplevert\n\nWerken aan een platform dat direct bijdraagt aan de energietransitie.\nGrote technische verantwoordelijkheid en ruimte om richting te bepalen.\nSamenwerken met ambitieuze data en ML experts binnen een groeiende data organisatie.\n\nWerk om trots op te zijn\n\nAls Lead ML Operations ben jij verantwoordelijk voor het technisch en operationeel eigenaarschap van ons ML/AI platform. Je borgt kwaliteit, stabiliteit en compliance, terwijl je het MLOps team inhoudelijk aanstuurt en ons platform continu doorontwikkelt. Zo speel je een cruciale rol in het realiseren van betrouwbare ML producten die Stedin helpen de energietransitie mogelijk te maken.\n\nEen dag als Lead ML Operations\n\nJe start met een sync met Data Office en bespreekt de nieuwste ontwikkelingen en impact daarvan op het platform.\nJe overlegt met het MLOps team over de nieuwe feature store opzet.\nJe stemt met het IT infra team af welke impact deze wijzigingen hebben op onze infrastructuur.\nJe luncht gezellig met collega’s buiten of in het bedrijfsrestaurant.\nJe werkt aan de kwartaalplanning met leads en product owners, beantwoordt mails en hebt een bila met een junior engineer.\n\nHier laad jij van op\n\nAlle financiële arbeidsvoorwaarden op een rij, weet je meteen waar je aan toe bent:\n\nEen bruto maandsalaris tussen € X.XXX,- en € X.XXX,- (bij 40 uur), 24,5 vakantiedagen en pensioenopbouw bij ABP waarbij wij 70% van de premie betalen.\nBovenop je salaris ontvang je een persoonlijk budget van 24% (incl. 8% vakantiegeld). In onze rekentool zie je hoe dit werkt.\nJaarlijks € 700,- (bruto) voor je eigen energie, zoals sport of vitaliteit.\nEen laptop en telefoon, of € 30,- netto per maand bij gebruik van je eigen telefoon.\n\nDaar komt dit nog bovenop\n\nModerne thuiswerkfaciliteiten plus € 2,40 thuiswerkvergoeding; we werken 50% thuis en 50% op kantoor.\nReizen met de ov vrijregeling of flexibele reisregeling – combineren mag.\nJouw ontwikkeling staat centraal: trainingen, opleidingen en events.\n\nVol energie vooruit met team Machine Learning\n\nJe komt te werken in het Machine Learning team binnen Data Office, een groeiende club experts op het gebied van Machine Learning, Artificial Intelligence en Data. We helpen elkaar verder met code reviews, technische sparsessies en veel samenwerking met business en IT teams. Je werkt afwisselend thuis en op kantoor in Rotterdam, waar een hechte en professionele sfeer hangt.\n\nMet deze kwaliteiten groei jij hier verder\n\nJe hebt visie, technisch leiderschap en kunt schakelen tussen strategie en detail. Je overziet afhankelijkheden, bewaakt kwaliteit en weet je team inhoudelijk te versterken.\n\nVerder Heb Je\n\nEen WO/HBO opleiding in Kunstmatige Intelligentie, Informatica of een vergelijkbaar vakgebied.\nMinimaal 5 jaar ervaring in data science en/of machine learning.\nExpert level skills in Python, Spark, Azure, Azure Data Lake Storage, Databricks, Kubernetes, Apache Airflow en MLOps tooling.\n\nZet de stap naar jouw volgende uitdaging\n\nWe kijken ernaar uit je te ontmoeten.\n\nDit Is Nog Goed Om Te Weten\n\nJe mag een motivatiebrief meesturen, dat hoeft niet.\nJe werkt in principe 40 uur per week; 32 uur is ook mogelijk.\nDe uiterste datum om te reageren voor deze vacature is 12 februari, reageer dus voor die tijd als je interesse hebt in de functie.",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Data Engineer BI Diverse Stafafdelingen",
      "company": "Nederlandse Spoorwegen",
      "location": "Utrecht, Utrecht, Netherlands (Remote)",
      "url": "https://www.linkedin.com/jobs/view/4368250536/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:12.302128",
      "description": "About the job\n\nWerken bij NS als data engineer betekent impact maken met data op duurzame mobiliteit en veiligheid, in een hecht multidisciplinair team, met sterke ontwikkelkansen.-\n\nDaarom wil je als Data Engineer werken bij NS\n\nAls data engineer bij NS werk je met moderne tools zoals Snowflake, Azure en Power BI binnen een gezellig scrumteam. Samen zorg je ervoor dat belangrijke informatie uit verschillende databronnen beschikbaar is voor dashboards, analyses en het nemen van goede beslissingen.\nJe draagt direct bij aan een data-gedreven NS en volop kansen om jezelf verder te ontwikkelen.\nMet je team maak je informatieproducten voor verschillende afdelingen, zoals Corporate Security en HRN CIS, waardoor je echt iets betekent voor het dagelijks reilen en zeilen bij NS.\n\n\"Jeroen: “Het mooiste aan deze functie is dat je met moderne data‑technologie direct bijdraagt aan miljoenen reizigers. Geen dag is hetzelfde, en in ons team helpen we elkaar continu groeien.”\"\n\nDit ga je doen als Data Engineer\n\nAls data engineer werk je samen met een multidisciplinair team, bestaande uit data engineers en informatieanalisten. Samen ontwikkel en onderhoud je informatieproducten voor diverse interne klanten binnen de Stafafdelingen van NS. Enkele voorbeelden hiervan zijn:\n\nCorporate Security: deze afdeling focust zich op de veiligheid van NS-terreinen en -gebouwen.\nInkoop: hier breng je in kaart waar NS geld aan uitgeeft, van een bos bloemen voor een zieke collega tot de aanschaf van een nieuwe trein.\nConcernhuisvesting: deze afdeling zorgt dat er parkeerplaatsen zijn voor het rijdend personeel en biedt ondersteuning, bijvoorbeeld als een koffieautomaat in storing raakt.\nHRN CIS (HoofdRailNet Concessie Informatie Systeem): NS heeft voor de concessieperiode 2025-2033 een informatiesysteem ingericht, waarin onder andere reizigerspunctualiteit en zitplaatskans per aankomststation worden vastgelegd en maandelijks gepubliceerd.\n\nAls Data Engineer werk je intensief samen met collega’s in de keten, zoals architecten, je eigen team, andere data teams binnen de afdeling en de business. Je vervult een essentiële rol in het bouw- en deliveryproces, waarbij je verantwoordelijk bent voor het ontwikkelen en in productie nemen van informatieproducten. Je denkt mee in oplossingen en stimuleert verbeteringen.\n\nJe bent door je leergierigheid, teamgeest, motivatie en je ‘aanpakken en doorpakken’-mentaliteit, van onschatbare waarde bij:\n\nHet ontsluiten van nieuwe databronnen, zowel gestructureerd als ongestructureerd.\nHet verwerken en modelleren van ontsloten data.\nHet ontwikkelen van datamarts op basis van deze data.\nHet uitvoeren van ETL/ELT-processen binnen Snowflake met SQL en Python.\nHet ontwerpen en realiseren van een effectief dataplatform van bron tot eindgebruiker.\nHet schrijven van robuuste code met controles voor verwerking en datakwaliteit.\nHet zorgen voor een vlekkeloze productielancering met geteste, gedocumenteerde oplossingen en monitoring.\nHet maken van dashboards en rapportages in Power BI, in nauwe samenwerking met gebruikers.\n\nHier ga je werken\n\nIn ons hoofdkantoor, op loopafstand van station Utrecht Centraal, of vanuit huis maak je deel uit van een zelf organiserend scrumteam binnen de afdeling Data, Innovatie & Analyse (DIA). Binnen DIA werkt een mix van ervaringen en persoonlijkheden, die elkaar graag opzoeken om met elkaar te sparren en kennis te delen. De afdeling geeft gebruikers betrouwbaar inzicht om tot juiste acties te komen door het verzamelen, combineren, analyseren en presenteren van informatie.\n\nBij NS vinden we diversiteit en inclusie belangrijk, het maakt samenwerken leuker en het resultaat wordt er vaak beter van. Wat jouw achtergrond of levensovertuiging is, maakt dan ook niet uit: we zijn vooral benieuwd naar jouw visie op de functie van Data Engineer.\n\nDit zijn de functie-eisen\n\nJe hebt een afgeronde HBO- of WO-opleiding en minstens drie jaar werkervaring als Data Engineer in een ETL/BI/DWH-omgeving.\nJe bent bekend met het visualiseren van data en hebt minstens drie jaar ervaring met Power BI, inclusief het werken met DAX en M-query.\nJe beschikt over aantoonbare kennis van object-, conceptueel en dimensioneel datamodelleren.\nJe hebt ruime ervaring met data-integratie, deployment automation, versiebeheer en ETL-processen, bijvoorbeeld met Azure Data Factory.\nJe hebt diepgaande kennis van Cloud DWH-oplossingen zoals Snowflake en SQL, en werkt vlot met Azure DevOps/GIT, CICD en Agile Scrum.\nJe bent communicatief sterk in het Nederlands, weet soepel te schakelen tussen business en techniek, neemt initiatief en betrekt anderen bij het behalen van doelen.\nJe bent kritisch, neemt verantwoordelijkheid voor sprintresultaten, ondersteunt je teamgenoten, en ziet het geven en ontvangen van feedback als een kans om samen te groeien.\n\nBij NS vinden we het belangrijk dat we elkaar durven aanspreken en ons altijd kunnen uitspreken. We zoeken collega’s met lef, die open staan voor verandering. Voor deze functie zijn dit a",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "BI Lead Business Data Engineer (NOT REMOTE)",
      "company": "Ricoh UK",
      "location": "Netherlands (Remote)",
      "url": "https://www.linkedin.com/jobs/view/4351594635/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:12.365197",
      "description": "About the job\n\nAbout The Role\n\nWe’re looking for a BI Lead Data Engineer to lead the design and delivery of our EMEA Supply Chain data and reporting infrastructure. You’ll build and oversee scalable, secure data pipelines and models using Microsoft Fabric and Azure, turning raw data into reliable, business-ready insights. This role bridges business and IT and plays a key role in how Ricoh makes data-driven decisions.\n\nWhat You will Do?\n\nDesign the data architecture across bronze, silver, gold layers.\nBuild and maintain data pipelines and semantic models in Azure and Microsoft Fabric as well as reports in PBI.\nEnsure data quality, governance, security, and performance.\nManage and mentor a small team of data engineers.\nCollaborate closely with Supply Chain, Finance, 3PLs, and IT teams.\nDrive the BI development roadmap and support reporting across 24 countries.\n\nWhat you bring?\n\n5+ years in BI/data engineering roles, with team leadership experience.preferably in an SC or manufacturing environment\nStrong knowledge of Azure Data Services, Microsoft Fabric, Power BI.\nIdeally experience with Snowflake, Phyton and DBT\nExperience in data modeling, governance, and pipeline development.\nAbility to work cross-functionally and translate business needs into data solutions.\nExcellent communication and problem-solving skills.\n\nWhat we offer:\n\nAs a BI Lead Data Engineer at Ricoh, you’ll enjoy:\n\nSalary based on experience (range: €4900 - €6800) with a performance-based bonus (max. 12%).\n25 vacation days + 6 ADV days\nTravel allowance and a solid pension scheme\nUnlimited access to e-learnings and opportunities for international growth\nAn innovative, international work environment focused on teamwork and continuous improvement\nA supportive, motivated team that values collaboration and impact\n\nWhy Ricoh?\n\nAt Ricoh, we empower digital workplaces with innovative technology. This is your chance to make a real impact in a collaborative, forward-thinking environment where data drives change across Europe.\n\nReady for the next step?\n\nIf you have any questions about the position, please contact Timothy (Recruiter) via\n\n📞 +31 6 15486346\n\n✉️timothy.van.thoff@ricoh.nl",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Data Engineer",
      "company": "WeTravel",
      "location": "Amsterdam Area (Hybrid)",
      "url": "https://www.linkedin.com/jobs/view/4369087594/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:12.429412",
      "description": "About the job\n\nHi! 👋 I’m Tara, BI Manager at WeTravel, based in the US 🇺🇸 I work closely with teams across Product, Finance, Sales, and Operations, and data sits at the center of almost everything we do. As WeTravel continues to grow globally, the complexity and importance of our data grows with it.\n\nWe’re looking for a curious, impact-driven Data Engineer to help us scale our data platform and make sure teams across the company can rely on clean, trustworthy, well-modeled data to do their best work.\n\nThis role is ideal for someone who enjoys building reliable systems, digging into messy data, and improving the foundations that analytics and decision-making depend on.\n\nAbout WeTravel\n\nWeTravel is building the leading booking and payments platform for group travel and multi-day tour operators.\n\nTravel organizers around the world use WeTravel to create trip proposals, manage bookings, and securely process payments, without relying on spreadsheets, emails, or fragmented tools. Our platform helps them focus on what matters most: creating meaningful travel experiences.\n\nAs WeTravel grows, data plays a critical role in how we understand our product, support our customers, and make better decisions across the business. From payments and revenue to product usage and subscriptions, reliable data is essential to everything we do.\n\nWhat’s the role\n\nThis Data Engineer role sits at the intersection of engineering and analytics, with a strong focus on data infrastructure, pipelines, and reliability.\n\nYou’ll work alongside our existing Data Engineer to design, build, and maintain the systems that move data from raw sources into our data warehouse, where it can be safely used by Analytics, Product, Finance, and Go-To-Market teams.\n\nThis is not an analytics or reporting role. Your primary responsibility is to ensure that data is accurate, available, and scalable, so others can confidently use it downstream.\n\nWhat you’ll do day-to-day\n\nDesign, build, and maintain scalable ELT/ETL pipelines ingesting data from product databases, third-party tools, and external partners\nOwn and evolve WeTravel’s data warehouse and data models to support analytics, dashboards, and downstream use cases\nEnsure data quality, reliability, and observability through testing, monitoring, documentation, and proactive issue detection\nSupport analytics and BI use cases related to subscriptions, payments, revenue, churn, and cohort analysis by delivering clean, well-structured data\nOptimize data performance, cost, and scalability as data volume and complexity grow\nEstablish and promote best practices for data modeling, naming conventions, and governance\nWork closely with Analytics, Product, Finance, and Engineering teams to translate data needs into durable, maintainable solutions\n\nWork is prioritized through a centralized process, so you can focus on execution and reliability rather than ad-hoc stakeholder requests.\n\nHow we work\n\nOur data team balances ownership with collaboration.\n\nYou’ll have autonomy to design and improve data infrastructure, while working closely with BI, Analytics, and Engineering partners to solve real business problems. We operate in bi-weekly sprints, prioritize work intentionally, and focus on building systems that are resilient and easy to maintain.\n\nWe care deeply about doing things the right way: clean data models, thoughtful documentation, and pipelines teams can trust. Your work will be visible, impactful, and used daily across the company.\n\nWe’re a growing team, still figuring things out, and we value people who are comfortable with that ambiguity and enjoy improving systems over time.\n\nYou should apply if you have\n\nBasic Qualifications\n\n3+ years of experience as a Data Engineer or in a role heavily focused on data pipelines and data warehouses\nStrong SQL skills and experience modeling data for analytics and reporting\nExperience with modern cloud data warehouses such as Snowflake, BigQuery, or Redshift\nFamiliarity with ELT tools like Fivetran or Airbyte, and transformation frameworks such as dbt\nProficiency in at least one programming language commonly used in data engineering, such as Python\nUnderstanding of data quality, testing, and monitoring concepts\nAbility to work independently, take initiative, and communicate clearly with non-technical partners\n\nNice to have\n\nExperience in SaaS, fintech, or subscription-based businesses\nFamiliarity with product analytics or experimentation data\nExposure to BI tools such as Tableau, Looker, or Mode\nExperience supporting metrics like MRR, churn, expansion, and cohort retention\nBackground in enabling self-serve analytics for business teams\n\nYou might not be the right fit if you\n\nAre primarily interested in analytics, dashboards, or reporting ownership\nPrefer highly structured environments with fixed processes and little ambiguity\nExpect to work mainly on greenfield architecture rather than improving existing systems\nAre uncomfortable owning production pipelines and being accountable ",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Senior Data EngineerSenior Data Engineer",
      "company": "Full Orbit",
      "location": "Haarlem, North Holland, Netherlands (Hybrid)",
      "url": "https://www.linkedin.com/jobs/view/4368305638/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:21.709071",
      "description": "About the job\n\nHeb jij een passie voor het ontwikkelen van dataoplossingen die daadwerkelijk bijdrage aan waarde voor de klant? Wil je meebouwen aan moderne datamanagement oplossingen en herken je zaken als Data Lake, Lakehouse en Data Warehouses? Ben je in staat om ML/AI concepten te vertalen naar daadwerkelijk werkende programmatuur?\n\nWij zijn op zoek naar een senior Data Engineer met een goed begrip van de verschillende concepten en een brede beheersing van relevante tools. Het gaat jou niet perse om de tools, maar om de oplossing. Heb jij een passie voor het vakgebied en een handson mentaliteit dan willen we je graag in ons team.\n\nWat ga je doen?\n\nAls Senior Data Engineer ontwerp, ontwikkel en beheer je geavanceerde data-oplossingen. Je zorgt ervoor dat onze klanten optimaal gebruikmaken van hun data. Je werkt aan:\n\nData ontsluiten uit diverse bronnen en het modelleren ervan\nHet opzetten van schaalbare infrastructuren met verschillende tools zoals MS Fabric, Azure Synapse, Databricks, Azure Data Factory, Oracle Data Integrator en Oracle OCI Data Integration\nHet creëren van krachtige inzichten met SQL, Python en visualisatie-tools zoals Power BI, Tableau en Qlik\nAdviseren over data-platformen en analytics\nCoachen van collega’s en bijdragen aan Agile-teams\nHybride werken: vanuit ons kantoor in Haarlem, bij de klant of thuis\n\nWat breng je mee?\n\nAfgeronde hbo- of wo-opleiding\nUitstekende beheersing van Nederlands en Engels\nMinimaal 5 jaar ervaring in een data-gedreven omgeving\nErvaring met Python, SQL, PL/SQL (pré)\nKennis van Data Factory, Data Fabric, Snowflake, Databricks en Oracle Data Integrator (pré)\nBekend met analytics-tools (Power BI, Tableau, Oracle Analytics)\nErvaring met cloudplatformen (Azure, AWS, GCP, Oracle OCI)\nInteresse/ervaring met AI (o.a. OpenAI, Oracle AI)\nKennis van data-opslagconcepten (Data Lake, Lakehouse, Data Mesh, etc.)\nErvaring met automatisering (CI/CD, Terraform, Ansible, scripting, API-first)\nBekwaam in data-modellering, governance, architectuur en security\n\nWat bieden we jou?\n\nAantrekkelijk salaris, afhankelijk van je vaardigheden en ervaring\nOpleidingen die aansluiten bij je carrierepad en regelmatig kennissessies\nEindjaarsbonus op basis van bedrijfsprestaties\nAantrekkelijke arbeidsvoorwaarden met keuzemogelijkheden\nMobiliteitsregeling: keuze voor leaseauto of vervoersbudget\nApple Laptop, mobiel abonnement\nTegemoetkoming bij de aanschaf van mobile en tablet\nNetto onkostenvergoeding\nThuiswerkregeling\nJaarlijks uitje",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Data EngineerData Engineer",
      "company": "Online Payment Platform",
      "location": "Delft, South Holland, Netherlands (Hybrid)",
      "url": "https://www.linkedin.com/jobs/view/4368734200/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:21.775165",
      "description": "About the job\n\n3 reasons you should join our team as Data Engineer\n\nData is your playground - and ours too. With over 10 million users and billions of transactions per month, your work directly influences the decisions that shape OPP’s future.\nYou won’t just build pipelines - you’ll help shape the foundation of a brand-new data warehouse from scratch. This is your chance to leave a mark on a critical company-wide initiative.\nJoin a fast-growing fintech with all the right ingredients: a collaborative culture, tech-minded peers, and the energy of a scale-up - backed by Worldline.\n\nWhat started as five people, a small office, and a few nerf guns has now grown into one of Europe’s fastest-growing fintechs. With 150+ colleagues, offices in Delft, Berlin, and London, and Worldline as a partner, we have the perfect mix: the resources of a global player with the energy of a scale-up. At OPP, we build the technology that powers the next generation of e-commerce. From eBay to Ikea, billions in transactions already flow through our systems. We don’t just handle payments—we enable platforms to accept, hold, and disburse funds securely.\n\nAs a Data Engineer, you’ll join our dedicated data team in Delft, working closely with the CTO, Data Analysts, a Data Engineer, and the DevOps/Development team. Your primary mission? Build and deliver the ongoing initiative for a robust and scalable data warehouse to support better, faster decision-making. You’ll design and optimize data pipelines, ensure data quality, support data governance, and automate infrastructure with Terraform. In the short term, your focus will be on getting our data warehouse live. In the long-term, you'll enable analytics and data-driven strategies that help us outperform competitors and stay fast on our feet.\n\nYour track record tells us\n\nYou bring at least 5 years of experience in data (platform) engineering, preferably within high-traffic or fintech environments.\nYou have an Bachelors degree in a relevant technical field.\nYour toolbox includes advanced SQL (including window functions), Terraform, Python (Airflow, PySpark), and AWS services (Athena, Kinesis, Lambda).\nYou have strong experience with developing and maintaining robust data ingestion pipelines with APIs, cloud services, and third-party systems.\nYou’re familiar with data modeling (star/snowflake schemas) and GitLab CI/CD.\nYou take ownership of end-to-end data pipelines, value collaboration with data analysts and developers, and troubleshoot like a pro.\nDetail-oriented, proactive, and a team player who thrives in cross-functional setups.\n\nOur offer\n\nA total yearly salary of up to €95.000.\nA modern, flexible workplace with hybrid working.\n24 vacation days and 2 supercharge-me-days ⚡.\n12-month contract with the possibility of permanent extension.\nThe tools you need: a MacBook, home office setup, and NS Business Card.\nAn attractive bike plan if you work 24 hours or more.\nA developers-only office at Lange Geer 26, within walking distance of our sales and product teams in the heart of Delft.\nA great work atmosphere with catered lunches and regular team drinks.\n\nFrom contact to contract\n\nWant to know more about our projects?\n\nSend your credentials our way and let’s talk. More info? Direct your questions to Gijs Cooijmans at gijs.cooijmans@onlinepaymentplatform.com. Or give him a WhatsApp message at +31 6 20019821.\n\n👋 it's me Mike, your hiring manager for this role\n\nDid you know our APIs are called millions of times a day? Will you help keep our platform lightning-fast? Let’s connect!",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Data EngineerData Engineer",
      "company": "SkillRecruit",
      "location": "Amsterdam, North Holland, Netherlands (Hybrid)",
      "url": "https://www.linkedin.com/jobs/view/4368718708/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:21.836415",
      "description": "About the job\n\nRole: Data Engineer\n\nLocations :Amsterdam ,Netherlands\n\nMinimum Experience: 8+\n\nType FTE\n\nSkill Set\n\nJava, spark, AWS, Web Services Rest API, Kafka, Data Engineering\n\nSkill to Evaluate\n\nJava, spark, Web Services Rest API, Kafka, Data Engineering, Snowflake, Python, SQL, AWS/Azure\n\nJob Description\n\nThe ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines that support our data analytics and machine learning initiatives. The Data Engineer will collaborate with cross-functional teams to ensure the availability, quality, and reliability of data across various platforms.",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Principal ML Systems Engineer | GCP · MLOps · Applied ML | B2B · EU Remote | €80K - €100K",
      "company": "Joppy",
      "location": "European Union (Remote)",
      "url": "https://www.linkedin.com/jobs/view/4368723457/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:21.901285",
      "description": "About the job\n\nOne of the companies we collaborate with at Joppy is looking for someone who doesn’t just train models, but builds and operates ML systems in production end-to-end.\n\nFull ownership: from understanding the business problem and engineering the data, to deploying, monitoring, and continuously improving models in real manufacturing and operational environments.\n\n🌍 Remote from anywhere in the EU\n\n✈️ One week per quarter in the USA (all expenses covered)\n\n🤝 B2B contract\n\n🔧 What you will actually do\n\nDesign predictive, prescriptive, and optimization models for maintenance, quality, and supply chain\nBuild data pipelines, feature pipelines, and inference services\nDeploy models as APIs, batch jobs, and streaming workloads in GCP\nImplement monitoring, drift detection, and retraining strategies\nTranslate ambiguous operational problems into measurable ML solutions\nWork closely with technical and operations teams\n\n🎯 Must-have\n\n10+ years across ML engineering, data engineering, and production systems\nProven experience delivering end-to-end ML systems beyond notebooks\nStrong background in classical ML and applied statistics (not LLM-centric)\nProduction-grade Python and strong SQL skills\nGCP experience: BigQuery, Dataflow/Beam, Airflow/Composer, Docker, Kubernetes\nHands-on MLOps, CI/CD, model versioning, and monitoring\nAbility to work under a B2B contract and travel to the USA quarterly\n\nAbout Joppy\n\nJoppy is a technology recruitment platform built for developers by developers.\n\n✅ No CV is required. Just say what you know and what you want.\n\n✅ Anonymous profile by default.\n\n✅ You choose who can talk to you. Companies cannot write to you until you accept their offer.\n\n✅ Only relevant offers that match your preferences. No more Javascript offers for Java developers.\n\n✅ Get rewarded if you get hired.\n\nKeep an eye on tech job opportunities anonymously and find the job that makes you happy.",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "AI Engineer",
      "company": "Royal Terberg Group",
      "location": "Houten, Utrecht, Netherlands (Hybrid)",
      "url": "https://www.linkedin.com/jobs/view/4346182717/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:21.965179",
      "description": "About the job\n\nAs an AI Engineer at Terberg Special Vehicles, you’ll work in a multidisciplinary product team on the innovative “Mechanic of the Future” platform.\n\nYour Impact\n\nDo you want to help shape the digital future of our service technicians? You’ll develop, integrate, and maintain advanced AI solutions that directly contribute to more efficient service, improved customer satisfaction, and a future-proof organization. You’ll have the freedom to pioneer, innovate, and truly make a difference within a family-owned company with international ambitions.\n\nWhat will you do?\n\nDesign, build, and maintain scalable data pipelines (batch and real-time) and retrieval-augmented generation (RAG) pipelines.\nIntegrate and manage data from diverse sources (cloud and on-premises), including setting up vector databases and embedding models.\nPerform data preprocessing, feature engineering, model training, and evaluation using Python, (No)SQL, (Py)Spark, Azure Data Factory, Databricks, and relevant AI/ML libraries.\nFine-tune and evaluate large language models (LLMs) for domain-specific tasks, including prompt engineering.\nApply CI/CD, version control, and documentation for structured and reproducible development.\nEnsure reliable, secure, and compliant data flows and AI models, following governance and ethical AI principles.\nMonitor and optimize the performance and cost-efficiency of data pipelines and AI models.\nCollaborate with software engineers, data scientists, product owners, and business stakeholders to deliver high-quality AI solutions.\nDocument model behavior, performance metrics, and decision logic for transparency and reproducibility.\n\nYour Profile\n\nBachelor’s or Master’s degree in a relevant field (Computer Science, Data Science, Software Engineering).\nAt least 3 years of experience as an AI/Data Engineer or similar role.\nExperience with Azure Data Factory, Databricks, Python, (No)SQL, (Py)Spark, and CI/CD.\nKnowledge of building and maintaining data pipelines, vector databases, and embedding models.\nProactive, independent, and strong communicator; you enjoy working in an agile team and take initiative.\nQuality, security, and compliance are important to you, and you are always looking for improvement and innovation.\n\nWhat do we offer?\n\nA key role in an innovative team shaping Terberg’s digital transformation.\nWork at a family-owned company with a friendly atmosphere, short lines of communication, and international ambitions.\nFriday afternoons off, excellent employment conditions according to the Metal & Technology Collective Labor Agreement.\nOpportunities for personal development via the Terberg Academy.\nHybrid working is possible.\nDirect influence on the future of our products and services.\n\nAs a colleague at Terberg, you will enjoy a great role at one of the most prominent and largest manufacturing companies in the Netherlands. We work from Monday to Friday morning, leaving Friday afternoons free. The atmosphere in our family-owned company is pleasant, and you’ll be part of a motivated team. We value your input, and your ideas are always heard. Staying up-to-date in your field is important to us; you’ll have access to numerous training and growth opportunities through the Terberg Academy. This position comes with an excellent benefits package and we adhere to the Metal and Technology Collective Labor Agreement (CAO).\n\nIs this your dreamjob?\n\nAre you the AI Engineer who will help us make the Mechanic of the Future a reality? Apply directly via our website or contact Bjorn Frank, 0031-6-42 52 27 74 for more information. You can also contact Diederik Stratenus at +31 6 51 49 12 05. We look forward to your contribution to our team!\n\nTerberg Special Vehicles is a global leader in the development, production, and service of specialized vehicles. As a family business, we have been committed to our people for over 150 years! Innovation is our strength, and sustainability is our answer. Since 2014, we have been investing in electrically powered vehicles and are actively testing hydrogen solutions. We aim to care for our environment responsibly so that future generations can also enjoy a good life on our planet.\n\nWe do not appreciate acquisition efforts in response to this vacancy. Unsolicited submissions of candidates will not be taken into consideration.",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Principal Data Engineer - PerfectScale by DoiT",
      "company": "DoiT",
      "location": "Netherlands (Remote)",
      "url": "https://www.linkedin.com/jobs/view/4325427560/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:22.031472",
      "description": "About the job\n\nLocation\n\nOur Principal Data Engineer will be an integral part of our Engineering teams in EMEA. This role is based remotely as a full-time employee in the UK, Ireland, Estonia, the Netherlands, Sweden, and Israel. We are also open to contractors in East Europe and Portugal.\n\nWho We Are\n\nDoiT is a global technology company that works with cloud-driven organizations to leverage the cloud to drive business growth and innovation. We combine data, technology, and human expertise to ensure our customers operate in a well-architected and scalable state - from planning to production.\n\nDelivering DoiT Cloud Intelligence, the only solution that integrates advanced technology with human intelligence, we help our customers solve complex multicloud problems and drive efficiency.\n\nWith decades of multicloud experience, we have specializations in Kubernetes, GenAI, CloudOps, and more. An award-winning strategic partner of AWS, Google Cloud, and Microsoft Azure, we work alongside more than 4,000 customers worldwide.\n\nAbout DoiT's PerfectScale Platform\n\nDoiT offers PerfectScale, a pioneering Kubernetes optimization and management solution that empowers DevOps, SRE, and Platform Engineering teams to optimize cloud performance while minimizing costs. We combine advanced AI technology with SME-human expertise to help organizations achieve peak Kubernetes efficiency.\n\nThe solution delivers a seamless onboarding experience, an intuitive UI, and a powerful autonomous optimization engine that ensures Kubernetes environments run efficiently with minimal human intervention.\n\nThe Opportunity\n\nAs a Principal Data Engineer, you will be both a hands-on contributor and a key architectural leader. You will design and build large-scale backend services and high-throughput data pipelines while also shaping the long-term technical direction of PerfectScale’s platform. This role combines deep technical ownership with active contribution to critical code, infrastructure, and performance-sensitive workloads.\n\nResponsibilities\n\nSystem Ownership: Design, build, and deploy large-scale distributed systems and high-throughput data pipelines using Go and cloud-native technologies.\nArchitecture & Code: Lead system-wide architectural decisions, focusing on data flow, performance, and resilience. Actively contribute to the codebase with high quality code.\nTechnical Leadership: Lead major technical initiatives, reduce technical debt and ensure the platform meets the reliability and scalability SLAs. Champion best engineering practices, code quality, testing and maintainability.\nCollaborate with product and engineering teams and R&D management to define the technical roadmap, review architecture and mentor junior engineers\n\nQualifications\n\nExperience: 8+ years of backend engineering experience, with 3+ years architecting high-load systems or data pipelines in a production environment.\nBackend Stack: Deep expertise in distributed systems using modern languages (Go, Java, Rust, or Python).\nData Systems: Strong, hands-on experience with relational and analytical databases (Postgres, ClickHouse is preferred).\nCloud-Native: Proven experience with microservices, containers, and modern DevOps practices (Docker, Kubernetes, GitOps, CI/CD).\nSkills: Demonstrated ability to combine hands-on coding with architectural leadership, including strong debugging, benchmarking, and performance optimization skills.\n\nBonus Points\n\nDeep Golang expertise\nDeep Kubernetes Knowledge\nExperience with modern data engineering technologies: Spark, Trino, Iceberg, Parquet, ClickHouse, DBT\nDBA background (relational, OLAP, columnar)\nExpertise in telemetry and time series\nCloud expertise (AWS, GCP, Azure)\n\nAre you a Do'er?\n\nBe your truest self. Work on your terms. Make a difference.\n\nWe are home to a global team of incredible talent who work remotely and have the flexibility to have a schedule that balances your work and home life. We embrace and support leveling up your skills professionally and personally.\n\nWhat does being a Do’er mean? We’re all about being entrepreneurial, pursuing knowledge, and having fun! Click here to learn more about our core values.\n\nSounds too good to be true? Check out our Glassdoor Page.\n\nWe thought so too, but we’re here and happy we hit that ‘apply’ button.\n\nFull-time employee benefits include:\n\nUnlimited PTO\nFlexible Working Options\nHealth Insurance\nParental Leave\nEmployee Stock Option Plan\nHome Office Allowance\nProfessional Development Stipend\nPeer Recognition Program\n\nMany Do'ers, One Team\n\nDoiT unites as Many Do'ers, One Team, where diversity is more than a goal—it's our strength. We actively cultivate an inclusive, equitable workplace, recognizing that each unique perspective enhances our innovation. By celebrating differences, we create an environment where every individual feels valued, contributing to our collective success.",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "IT Data Engineer",
      "company": "ABN AMRO Bank N.V.",
      "location": "Amsterdam, North Holland, Netherlands (Hybrid)",
      "url": "https://www.linkedin.com/jobs/view/4361501776/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:22.161761",
      "description": "About the job\n\nAt a glance\n\nAre you a technical expert and enthusiastic person who inspires others to create innovative solutions? Then we need your help, in making ABN AMRO a personal bank in a digital age!\n\nHow are you going to shape innovation, advancements, and projects within your department? Our employees get the freedom to grow and experiment. You support and guide them, so they can become the technical experts that contribute to a future-proof bank. We work hybrid, which means you and your team can flexibly decide where and when you can work most comfortable and effectively (either working from Home or Office).\n\nAll-in-all, working at ABN AMRO means that you contribute to projects that matter and impact millions of customers! Are you up for the challenge?\n\nYour job\n\nYou will be part of a team specifically focused on future-proof technologies and frameworks to help accelerate the bank’s strategy and initiatives.\n\nIn this role, you will oversee designing, developing, and implementing ETL frameworks & data pipelines that helps other teams deliver their data to their consumers with the main goal of being efficient and cost effective. You will be responsible for developing and reviewing software solutions that are deployed in Azure Cloud using a variety of technologies such as: Databricks, Kafka, Airflow etc.\n\nYou will be part of diverse Agile/Scrum DevOps team and have end-to-end responsibility, for developing, managing, and maintaining functionalities in the Credit Capability Team area.\n\nWorking environment\n\nAt ABN AMRO, we strive to be a personal bank in a digital age. That strategy is also brought to life in how we work ourselves. Hence, we offer hybrid working as the new normal. This offers you the flexibility to decide when and where can work most comfortably and effectively. Of course, we offer you the right resources you need to set up a good home office. For example, an ergonomic chair, desk, and laptop.\n\nYour profile\n\nYou feel at home in a technical environment and are always up to date on the latest technologies. Do you think you're a fit? Check your profile and apply if you recognize yourself (partially) in them:\n\nDevelopment skills:\n\nAtleast 7 years of experience working with Azure.\nExperience with Python, Pyspark, Typescript and willingness to learn other programming languages\nExperience with Databricks, Azure Devops.\nExperience with Airflow.\nKnowledge on streaming technologies (Kafka) is highly desireable.\nExperience with deploying and creating Azure infrastructure. (Biceps or Terraform)\n\nOps skills:\n\nKnowledge of change and incident management for e.g. in Servicenow.\n\nSoft skills:\n\nEnglish proficiency\nWorking experience with Scrum/Agile principles\nGood communicator.\n\nWe are offering\n\nYou are given every opportunity and independence to demonstrate your professional expertise in a no-nonsense environment and to further develop yourself. We attach great value to personal development and thereby offer you many training and development opportunities. In addition:\n\nA good monthly salary based on a 36-40 hour working week.\nThe Benefit Budget is 11% of your salary. The Benefit Budget allows you to acquire additional employment benefits. If you make no purchases or reservations in the Benefit Shop in a given month, you are paid one twelfth of your Benefit Budget that month.\nFive weeks of vacation per year. You have the option to purchase an additional four weeks per year.\nPersonal development Budget of € 1,000 per year, which you can accumulate up to € 3,000.\nPossibility to work from home (in consultation with your team and depending on your position).\nAn annual public transport pass with free public transportation throughout the Netherlands.\nAn excellent pension scheme.\n\nInterested?\n\nAre you interested? Then respond online to this vacancy. For more information, you can contact Ramzi Alashabi (Chapter Lead Data) at ramzi.alashabi@nl.abnamro.com. We would like to get to know you.\n\nYour Future: Inclusive, Innovative, Sustainable\n\nDisclaimer external recruitment agencies",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Machine Learning Research Engineer",
      "company": "Understanding Recruitment",
      "location": "European Union (Remote)",
      "url": "https://www.linkedin.com/jobs/view/4358802550/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:22.227242",
      "description": "About the job\n\nMachine Learning Research Engineer\n\nAbout Unitary\n\nUnitary is a fast-growing AI startup building Virtual Agents that combine deterministic systems, LLM reasoning, and human-in-the-loop expertise to automate complex trust & safety workflows. Our platform processes millions of images and videos daily for enterprise customers, enabling safer and more reliable decisions at scale.\n\nBacked by Creandum and Plural, with $25M+ raised, we’re scaling quickly and defining the future of intelligent, accountable automation online.\n\nThe Role\n\nWe’re hiring a Machine Learning Research Engineer to shape the next generation of Virtual Agents. You’ll lead the development of an “Agent Factory” — a system that transforms captured workflows into scalable, production-ready agents by unifying code, orchestration, and AI reasoning.\n\nWorking at the intersection of research and engineering, you’ll prototype, validate, and productionise systems that allow AI to autonomously construct and optimise complex software workflows.\n\nWhat You’ll Do\n\nDesign and build the Agent Factory to create, deploy, and manage Virtual Agents\nDevelop frameworks for code generation, evaluation, and workflow automation\nIntegrate LLM-based reasoning with deterministic Python systems\nRun experiments to benchmark and improve automation quality\nPartner with platform, ML, and customer teams to deploy at scale\nInfluence Unitary’s technical roadmap and agentic AI research direction\n\nAbout You\n\nStrong Python expertise with experience in LLMs, Agentic AI, or code-generation systems\nSolid engineering foundations across software design, testing, and MLOps/DevOps\nComfortable moving between research, prototyping, and production systems\nCurious, pragmatic, and highly collaborative in fast-moving environments\n\nBonus: Experience with workflow orchestration (Temporal), browser automation (Playwright),\n\nCI/CD, Terraform, or scaling ML systems in production.\n\nWhy Unitary\n\nRemote-first across Europe\nCompetitive salary + meaningful equity\nFlexible working and generous parental/sick leave\nAnnual budgets for learning and wellbeing\nThree team off-sites per year across London or Europe",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "AI Engineer",
      "company": "KPN",
      "location": "Rotterdam, South Holland, Netherlands (Hybrid)",
      "url": "https://www.linkedin.com/jobs/view/4361333913/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:22.293069",
      "description": "About the job\n\nBen jij een ervaren AI-professional met een passie voor Generatieve AI en automatisering? Sluit je aan bij het AI for Corporate team van KPN\n\nJouw rol als AI Engineer\n\nBen jij een AI-engineer die net zo makkelijk met business-stakeholders schakelt als met technische teams? Iemand die energie krijgt van co-creëren met de business, processen doorgronden, kansen spotten én oplossingen bedenken en bouwen? Dan zoeken we jou! Als Medior AI Engineer in ons AI for Corporate team:\n\nOntwerp en bouw je Generatieve AI- en NLP-oplossingen die interne processen in HR & Finance slimmer, sneller en AI-Native maken.\nWerk je aan concrete use-cases zoals AI-agents voor HR/Finance, geautomatiseerd factuurscannen, goedkeuringsworkflows en voorspellende modellen.\nNeem je zelfstandig eigenaarschap over features — van idee tot productie — met focus op schaalbare, betrouwbare en maintainable AI-services.\nBen je een brugbouwer tussen tech en non-tech, durf je te challengen, stel je kritische vragen en bouw je sterke domein-connectie en begrip op.\nCombineer je AI-expertise met productie-mindset en software engineering-kwaliteit.\n\nTeam & werkmodel\n\nWe werken hybride met flexibele thuiswerk-opties. We ontmoeten elkaar op maandagen in Rotterdam en op donderdagen in Amsterdam. Daarnaast organiseren we regelmatig AI-knowledge-sharing en community events.\n\nDit is je team\n\nCorporate Insights & Analytics (CIA) is de data- en AI-motor binnen KPN. Met zo’n 40 professionals — van developers en data scientists tot consultants, data engineers, scrum masters en product owners — werken we verspreid over vijf Scrum-teams aan interne digitale innovatie. Je wordt onderdeel van het AI for Corporate team binnen Group Business Services, een hechte en ondernemende groep van zes AI-specialisten die AI-oplossingen bouwt voor HR & Finance. Binnen dezelfde afdeling werk je ook nauw samen met het Anaplan-, Oracle Reporting-, Managed Dashboarding- en Data-team én met KPN’s snelgroeiende AI-community van 50+ AI-engineers en data scientists.\n\nWat breng\n\njij ons?\n\nJe bent een Medior AI Engineer die technologie én business-impact laat samenkomen. Je werkt zelfstandig, neemt initiatief en krijgt energie van het oplossen van echte corporate uitdagingen binnen HR & Finance. Dankzij je consultant-uitstraling en engineering-achtergrond bouw je vertrouwen op bij stakeholders en vertaal je ideeën naar AI-oplossingen die schaalbaar, betrouwbaar en productie-ready zijn — precies waar Corporate Insights & Analytics (CIA) voor staat.\n\nDit breng je mee:\n\nEen afgeronde master in AI of Computer Science\n2–5 jaar ervaring met AI/ML-oplossingen in productie-omgevingen (bij voorkeur binnen enterprise-domeinen zoals CIA)\nConsultant mindset of uitstraling: je werkt graag samen met de business om oplossingen te bedenken én te bouwen\nSterke software engineering-basis met ervaring in production-grade Python\nErvaring met Docker en container-based development\nCloud-ervaring, bij voorkeur met Azure\nBekend met API-integraties en CI/CD-deployment flows\nNLP-ervaring of pipelines is een plus\nGewend om te schakelen tussen tech- en non-tech-teams, stakeholders te challengen en domein-begrip op te bouwen\n\nJouw kracht in samenwerking:\n\nJe denkt vanuit mogelijkheden én concrete oplossingen, niet alleen modellen\nJe communiceert helder met developers, data scientists en zakelijke stakeholders binnen CIA\nJe bent analytisch scherp, pragmatisch in je aanpak en technisch sterk in uitvoering\nJe bouwt connectie met het domein om oplossingen te maken die echt landen\n\nWaarom jij bij CIA past:\n\nJe bent niet alleen iemand die bouwt — je bent iemand die begrijpt, verbindt, adviseert én realiseert. Binnen Corporate Insights & Analytics (CIA) krijg je de ruimte om jouw expertise om te zetten in AI-services die interne processen verbeteren en meetbare waarde leveren aan de organisatie.\n\nKlaar voor impact binnen CIA? Sluit je aan bij ons team en help KPN’s interne dienstverlening AI-Native en toekomstbestendig te maken.\n\nGoede beheersing van de Nederlandse taal is vereist voor deze functie/ Proficiency in Dutch (spoken and written) is required for this role.\n\nWat krijg je\n\nvan ons?\n\nNatuurlijk hebben we als werkgever jou ook veel te bieden. Dit krijg je van ons:\n\nEen bruto maandsalaris van minimaal € 5.038,- en maximaal € 7.575,- afhankelijk van je werkervaring, op basis van 40 uur\nVaste 13e maand, een deel krijg je maandelijks uitbetaald en een deel in januari van het volgende jaar. Óf je zet het in voor een FLEX & BOOST doel naar keuze! Klik hier om er meer over te lezen\nJaarlijks een individuele en collectieve verhoging conform onze CAO\n€ 1500 per jaar aan inzetbaarheidsbudget dat je naar eigen inzicht mag gebruiken voor bijvoorbeeld trainingen, coaching en cursussen\nFlexibele vrije dagen: onze collega’s mogen zelf (in samenspraak met de manager) bepalen hoe zij met verlof omgaan\nEen goede werk-privé balans (zoals 2 dagen kantoor, 3 dagen thuis, transitieverlof, feestdagwissel naar een feestdag van",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    },
    {
      "title": "Data Engineer",
      "company": "Nationale-Nederlanden",
      "location": "The Hague, South Holland, Netherlands (Hybrid)",
      "url": "https://www.linkedin.com/jobs/view/4368713311/",
      "source": "LinkedIn",
      "scraped_at": "2026-02-04T21:35:22.358093",
      "description": "About the job\n\nIs het jouw passie om op basis van data waarde toe te voegen voor zowel de klant als jouw collega’s? Ben jij een topper in het bouwen van data pipelines en sta je open voor veranderingen en nieuwe technieken? Wil jij direct resultaat zien van jouw acties? Dan ben jij de Data Engineer die wij zoeken!\n\nAls Data Engineer binnen het schade intermediaire bedrijf ben je verantwoordelijk voor de vertaalslag van ruwe data naar gestructureerde bestanden. Je modelleert data en je zet pipelines op om de data geautomatiseerd te bewegen naar dashboards, voorspelmodellen of andere toepassingen. Samen met onze collega’s van pricing, product management en het data driven underwriting team zorg je voor juiste tarieven, het sturen op het rendement en de groei van de portefeuille. Je bent innoverend en maakt gebruik van de nieuwste tools en technieken.\n\nWat je gaat doen:\n\nHet modelleren van veel verschillende databronnen in SQL en Python, waarbij je werkt met de modernste technieken op het Azure data platform\nHet opzetten van pipelines om grote hoeveelheden data te combineren en te verzamelen in een Azure Data Lake\nHet creëren van een gebruikersomgeving op het Azure data platform voor de pricing actuarissen, data analisten en data scientists waar de juiste data te vinden is\nHet automatiseren van processen en data flows, hierbij gebruik makend van nieuwe technieken\n\nWat wij jou bieden\n\nNN investeert in een inclusieve, inspirerende werkomgeving en in vaardigheden en competenties voor de toekomst. Daarbij passen arbeidsvoorwaarden die aansluiten bij wat er vandaag nodig is én die rekening houden met de toekomst. Zo bieden we onze medewerkers de mogelijkheid het beste uit zichzelf te halen. Wij bieden jou:\n\nSalaris tussen € 3.517 en € 5.025 op basis van 36 uur, afhankelijk van jouw kennis en ervaring\n13de maand en vakantiegeld wordt maandelijks uitbetaald bij je salaris\n27 vakantiedagen bij een werkweek van 5 dagen en drie Diversiteitsdagen\nEen moderne pensioenregeling uitgevoerd door BeFrank\nVolop training- en opleidingsmogelijkheden\nNS Business Card 2e klas, waarmee je onbeperkt kunt reizen, ook privé. Reis je liever met eigen vervoer? Declareer dan je kilometers\nThuiswerkvergoedingen voor internet en een goede werkplek\n\nWij werken hybride bij NN (twee dagen per week op kantoor). De officiële locatie van deze functie is Den Haag.\n\nWat je meebrengt\n\nEen afgeronde (technische) HBO of WO opleiding in de richting econometrie, wiskunde, natuur/sterrenkunde, informatica of een gerelateerde studie zoals data science\nKennis van programmeertalen, zoals SQL, Python of vergelijkbaar en ervaring met het Azure platform, Git of CI/CD pipelines is een pré\nAffiniteit in het werken met big data en om dit toe te passen in een commerciële omgeving\nOngeveer 0-5 jaar relevante werkervaring in een gerelateerd veld\nJe bent bereid en in staat om nieuwe technieken te leren in een dynamische omgeving\nJe bent analytisch en statistisch sterk, werkt nauwkeurig en pragmatisch met duidelijke implementatiekracht\nJe bent proactief, innovatief en besluitvaardig in aanpak en uitvoering\n\nMet wie je werkt\n\nHet Pricing en Data team bestaat uit ongeveer 20 data engineers, data scientisten en actuarissen en zorgt voor de juiste data en tarieven. Het team heeft een centrale rol binnen het schade intermediaire bedrijf en werkt veel samen met andere teams binnen de afdeling, maar ook met de operationele teams, marketing en IT.\n\nHeb je vragen?\n\nVoor vragen over de procedure kun je terecht bij Liselotte van der Zeeuw – Senior Talent Acquisition Specialist Tech & Data via liselotte.van.der.zeeuw1@nn-group.com.\n\nHet gaat hier om een permanente positie. Een kandidaat ontvangt een contract van NN en we staan niet open voor interim/freelance opdrachten en/of kandidaten of acquisitie door bureaus. Alvast bedankt voor je begrip.\n\nWe bieden je\n\nNS Business card, ook privé te gebruiken\n27 vakantiedagen en 1 Diversiteitsdag\n13e maand en vakantiegeld\nHybride werken",
      "search_profile": "ml_data",
      "search_query": "ml_data"
    }
  ]
}